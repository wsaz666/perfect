import os

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch
import faiss
import numpy as np
from llama_index.core.node_parser import SentenceSplitter
import re
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import tempfile
import shutil
from typing import Optional
from openai import OpenAI
import gradio as gr
import fitz  # PyMuPDF
import chardet  # ç”¨äºè‡ªåŠ¨æ£€æµ‹ç¼–ç 
import hashlib
import traceback
from config import Config  # å¯¼å…¥é…ç½®æ–‡ä»¶
from text2vec import TextVector
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader

VERILOG_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ FPGA/ASIC å·¥ç¨‹å¸ˆï¼Œç²¾é€š Verilog å’Œ SystemVerilogã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œæä¾›çš„å‚è€ƒä»£ç ï¼ˆContextï¼‰ï¼Œç¼–å†™é«˜è´¨é‡ã€å¯ç»¼åˆçš„ Verilog ä»£ç ã€‚

è¦æ±‚ï¼š
1. **ä»£ç ä¼˜å…ˆ**ï¼šç›´æ¥è¾“å‡ºä»£ç ï¼Œé™¤éç”¨æˆ·è¦æ±‚è§£é‡Šã€‚
2. **è§„èŒƒæ€§**ï¼šå¿…é¡»åŒ…å« module å®šä¹‰ã€è¾“å…¥è¾“å‡ºç«¯å£ã€parameter å®šä¹‰ã€‚
3. **æ—¶åºé€»è¾‘**ï¼šå¤„ç†å¤ä½é€»è¾‘ï¼ˆé€šå¸¸æ˜¯å¼‚æ­¥ä½ç”µå¹³å¤ä½æˆ–åŒæ­¥é«˜ç”µå¹³å¤ä½ï¼‰ã€‚
4. **é£æ ¼ä¸€è‡´**ï¼šå‚è€ƒ Context ä¸­çš„å‘½åè§„èŒƒï¼ˆå¦‚ _i, _o, _rï¼‰å’Œç¼©è¿›é£æ ¼ã€‚
5. **æ³¨é‡Š**ï¼šå…³é”®é€»è¾‘éœ€è¦æ·»åŠ ç®€çŸ­æ³¨é‡Šã€‚
"""

# åˆ›å»ºçŸ¥è¯†åº“æ ¹ç›®å½•å’Œä¸´æ—¶æ–‡ä»¶ç›®å½•
KB_BASE_DIR = Config.kb_base_dir
os.makedirs(KB_BASE_DIR, exist_ok=True)

# åˆ›å»ºé»˜è®¤çŸ¥è¯†åº“ç›®å½•
DEFAULT_KB = Config.default_kb
DEFAULT_KB_DIR = os.path.join(KB_BASE_DIR, DEFAULT_KB)
os.makedirs(DEFAULT_KB_DIR, exist_ok=True)

# åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
OUTPUT_DIR = Config.output_dir
os.makedirs(OUTPUT_DIR, exist_ok=True)

print("ğŸ‘‰ æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹ï¼Œè¯·ç¨å€™...", flush=True)
global_vector_model = TextVector(Config())
print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼", flush=True)

client = OpenAI(
    api_key=Config.llm_api_key,
    base_url=Config.llm_base_url
)


class DeepSeekClient:
    def generate_answer(self, system_prompt, user_prompt, model=Config.llm_model):
        response = client.chat.completions.create(
            model=Config.llm_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            stream=False
        )
        return response.choices[0].message.content.strip()


# è·å–çŸ¥è¯†åº“åˆ—è¡¨
def get_knowledge_bases() -> List[str]:
    """è·å–æ‰€æœ‰çŸ¥è¯†åº“åç§°"""
    try:
        if not os.path.exists(KB_BASE_DIR):
            os.makedirs(KB_BASE_DIR, exist_ok=True)

        kb_dirs = [d for d in os.listdir(KB_BASE_DIR)
                   if os.path.isdir(os.path.join(KB_BASE_DIR, d))]

        # ç¡®ä¿é»˜è®¤çŸ¥è¯†åº“å­˜åœ¨
        if DEFAULT_KB not in kb_dirs:
            os.makedirs(os.path.join(KB_BASE_DIR, DEFAULT_KB), exist_ok=True)
            kb_dirs.append(DEFAULT_KB)

        return sorted(kb_dirs)
    except Exception as e:
        print(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}")
        return [DEFAULT_KB]


# åˆ›å»ºæ–°çŸ¥è¯†åº“
def create_knowledge_base(kb_name: str) -> str:
    """åˆ›å»ºæ–°çš„çŸ¥è¯†åº“"""
    try:
        if not kb_name or not kb_name.strip():
            return "é”™è¯¯ï¼šçŸ¥è¯†åº“åç§°ä¸èƒ½ä¸ºç©º"

        # å‡€åŒ–çŸ¥è¯†åº“åç§°ï¼Œåªå…è®¸å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œä¸­æ–‡
        kb_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', kb_name.strip())

        kb_path = os.path.join(KB_BASE_DIR, kb_name)
        if os.path.exists(kb_path):
            return f"çŸ¥è¯†åº“ '{kb_name}' å·²å­˜åœ¨"

        os.makedirs(kb_path, exist_ok=True)
        return f"çŸ¥è¯†åº“ '{kb_name}' åˆ›å»ºæˆåŠŸ"
    except Exception as e:
        return f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}"


# åˆ é™¤çŸ¥è¯†åº“
def delete_knowledge_base(kb_name: str) -> str:
    """åˆ é™¤æŒ‡å®šçš„çŸ¥è¯†åº“"""
    try:
        if kb_name == DEFAULT_KB:
            return f"æ— æ³•åˆ é™¤é»˜è®¤çŸ¥è¯†åº“ '{DEFAULT_KB}'"

        kb_path = os.path.join(KB_BASE_DIR, kb_name)
        if not os.path.exists(kb_path):
            return f"çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨"

        shutil.rmtree(kb_path)
        return f"çŸ¥è¯†åº“ '{kb_name}' å·²åˆ é™¤"
    except Exception as e:
        return f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {str(e)}"


# è·å–çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨
def get_kb_files(kb_name: str) -> List[str]:
    """è·å–æŒ‡å®šçŸ¥è¯†åº“ä¸­çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        kb_path = os.path.join(KB_BASE_DIR, kb_name)
        if not os.path.exists(kb_path):
            return []

        # è·å–æ‰€æœ‰æ–‡ä»¶ï¼ˆæ’é™¤ç´¢å¼•æ–‡ä»¶å’Œå…ƒæ•°æ®æ–‡ä»¶ï¼‰
        files = [f for f in os.listdir(kb_path)
                 if os.path.isfile(os.path.join(kb_path, f)) and
                 not f.endswith(('.index', '.json'))]

        return sorted(files)
    except Exception as e:
        print(f"è·å–çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        return []


# è¯­ä¹‰åˆ†å—å‡½æ•°
def semantic_chunk(text: str, file_ext: str, chunk_size=2000, chunk_overlap=20) -> List[str]:
    """
    è¯­ä¹‰åˆ†å—ï¼šåªè´Ÿè´£åˆ‡åˆ†ï¼Œè¿”å›å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
    IDç”Ÿæˆé€»è¾‘ä¸‹æ²‰åˆ° process_and_index_files ä¸­ä»¥ä¿è¯å…¨å±€å”¯ä¸€æ€§ã€‚
    """
    # 1. PDF/é€šç”¨ åˆ‡åˆ†å™¨
    pdf_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap,
        separators=["\n\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", "\n", " ", ""],
        keep_separator=True
    )
    # 2. TXT åˆ‡åˆ†å™¨ (ç‰¹æ®Šæ ‡è®°)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap,
        separators=["<|endoftext|>","\n\n", "\n", ";", ""],
        keep_separator=True
    )
    # 3. ä»£ç  åˆ‡åˆ†å™¨ (Verilog)
    code_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap,
        separators=["endmodule", "\n\n", "\n", ";", ""],
        keep_separator=True
    )

    file_ext = file_ext.lower()
    chunks = []

    if file_ext == ".pdf":
        chunks = pdf_splitter.split_text(text)
    elif file_ext == ".txt":
        chunks = text_splitter.split_text(text)
    elif file_ext in [".v", ".sv"]:
        chunks = code_splitter.split_text(text)
    else:
        chunks = pdf_splitter.split_text(text)

    # è¿‡æ»¤æ‰è¿‡çŸ­çš„å—
    return [c for c in chunks if len(c) >= 20]


# æ„å»ºFaissç´¢å¼•
def build_faiss_index(vector_file, index_path, metadata_path):
    try:
        with open(vector_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        if not data:
            raise ValueError("å‘é‡æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶ã€‚")

        # ç¡®è®¤æ‰€æœ‰æ•°æ®é¡¹éƒ½æœ‰å‘é‡
        valid_data = []
        for item in data:
            if 'vector' in item and item['vector']:
                valid_data.append(item)
            else:
                print(f"è­¦å‘Š: è·³è¿‡æ²¡æœ‰å‘é‡çš„æ•°æ®é¡¹ ID: {item.get('id', 'æœªçŸ¥')}")

        if not valid_data:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å‘é‡æ•°æ®ã€‚")

        # æå–å‘é‡
        vectors = [item['vector'] for item in valid_data]
        vectors = np.array(vectors, dtype=np.float32)

        if vectors.size == 0:
            raise ValueError("å‘é‡æ•°ç»„ä¸ºç©ºï¼Œè½¬æ¢å¤±è´¥ã€‚")

        # æ£€æŸ¥å‘é‡ç»´åº¦
        dim = vectors.shape[1]
        n_vectors = vectors.shape[0]
        print(f"æ„å»ºç´¢å¼•: {n_vectors} ä¸ªå‘é‡ï¼Œæ¯ä¸ªå‘é‡ç»´åº¦: {dim}")

        # ç¡®å®šç´¢å¼•ç±»å‹å’Œå‚æ•°
        max_nlist = n_vectors // 39
        nlist = min(max_nlist, 128) if max_nlist >= 1 else 1

        if nlist >= 1 and n_vectors >= nlist * 39:
            print(f"ä½¿ç”¨ IndexIVFFlat ç´¢å¼•ï¼Œnlist={nlist}")
            quantizer = faiss.IndexFlatIP(dim)
            index = faiss.IndexIVFFlat(quantizer, dim, nlist)
            if not index.is_trained:
                index.train(vectors)
            index.add(vectors)
        else:
            print(f"ä½¿ç”¨ IndexFlatIP ç´¢å¼•")
            index = faiss.IndexFlatIP(dim)
            index.add(vectors)

        faiss.write_index(index, index_path)
        print(f"æˆåŠŸå†™å…¥ç´¢å¼•åˆ° {index_path}")

        # åˆ›å»ºå…ƒæ•°æ®
        metadata = [{'id': item['id'], 'chunk': item['chunk'], 'method': item['method']} for item in valid_data]
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
        print(f"æˆåŠŸå†™å…¥å…ƒæ•°æ®åˆ° {metadata_path}")

        return True
    except Exception as e:
        print(f"æ„å»ºç´¢å¼•å¤±è´¥: {str(e)}")
        traceback.print_exc()
        raise


# å‘é‡åŒ–æ–‡ä»¶å†…å®¹
def vectorize_file(data_list, output_file_path, field_name="chunk"):
    """å‘é‡åŒ–æ–‡ä»¶å†…å®¹ï¼Œå¤„ç†é•¿åº¦é™åˆ¶å¹¶ç¡®ä¿è¾“å…¥æœ‰æ•ˆ"""
    if not data_list:
        print("è­¦å‘Š: æ²¡æœ‰æ•°æ®éœ€è¦å‘é‡åŒ–")
        with open(output_file_path, 'w', encoding='utf-8') as outfile:
            json.dump([], outfile, ensure_ascii=False, indent=4)
        return

    # å‡†å¤‡æŸ¥è¯¢æ–‡æœ¬ï¼Œç¡®ä¿æ¯ä¸ªæ–‡æœ¬æœ‰æ•ˆä¸”é•¿åº¦é€‚ä¸­
    valid_data = []
    valid_texts = []

    for data in data_list:
        text = data.get(field_name, "")
        # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©ºä¸”é•¿åº¦åˆé€‚
        if text and 1 <= len(text) <= 8000:  # ç•¥å°äºAPIé™åˆ¶çš„8192ï¼Œç•™å‡ºä¸€äº›ä½™é‡
            valid_data.append(data)
            valid_texts.append(text)
        else:
            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œæˆªæ–­å®ƒ
            if len(text) > 8000:
                truncated_text = text[:8000]
                print(f"è­¦å‘Š: æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³8000å­—ç¬¦ã€‚åŸå§‹é•¿åº¦: {len(text)}")
                data[field_name] = truncated_text
                valid_data.append(data)
                valid_texts.append(truncated_text)
            else:
                print(f"è­¦å‘Š: è·³è¿‡ç©ºæ–‡æœ¬æˆ–é•¿åº¦ä¸º0çš„æ–‡æœ¬")

    if not valid_texts:
        print("é”™è¯¯: æ‰€æœ‰æ–‡æœ¬éƒ½æ— æ•ˆï¼Œæ— æ³•è¿›è¡Œå‘é‡åŒ–")
        with open(output_file_path, 'w', encoding='utf-8') as outfile:
            json.dump([], outfile, ensure_ascii=False, indent=4)
        return

    # å‘é‡åŒ–æœ‰æ•ˆæ–‡æœ¬
    vectors = vectorize_query(valid_texts)

    # æ£€æŸ¥å‘é‡åŒ–æ˜¯å¦æˆåŠŸ
    if vectors.size == 0 or len(vectors) != len(valid_data):
        print(
            f"é”™è¯¯: å‘é‡åŒ–å¤±è´¥æˆ–å‘é‡æ•°é‡({len(vectors) if vectors.size > 0 else 0})ä¸æ•°æ®æ¡ç›®({len(valid_data)})ä¸åŒ¹é…")
        # ä¿å­˜åŸå§‹æ•°æ®ï¼Œä½†ä¸å«å‘é‡
        with open(output_file_path, 'w', encoding='utf-8') as outfile:
            json.dump(valid_data, outfile, ensure_ascii=False, indent=4)
        return

    # æ·»åŠ å‘é‡åˆ°æ•°æ®ä¸­
    for data, vector in zip(valid_data, vectors):
        data['vector'] = vector.tolist()

    # ä¿å­˜ç»“æœ
    with open(output_file_path, 'w', encoding='utf-8') as outfile:
        json.dump(valid_data, outfile, ensure_ascii=False, indent=4)

    print(f"æˆåŠŸå‘é‡åŒ– {len(valid_data)} æ¡æ•°æ®å¹¶ä¿å­˜åˆ° {output_file_path}")


# å‘é‡åŒ–æŸ¥è¯¢ - é€šç”¨å‡½æ•°ï¼Œè¢«å¤šå¤„ä½¿ç”¨
def vectorize_query(query, model_name=Config.model_name, batch_size=Config.batch_size) -> np.ndarray:
    """
    å‘é‡åŒ–æ–‡æœ¬æŸ¥è¯¢ï¼ˆå·²ä¿®æ”¹ä¸ºè°ƒç”¨ text2vec æœ¬åœ°æ¨¡å‹ï¼‰
    """
    if not query:
        return np.array([])

    if isinstance(query, str):
        query = [query]

    # è¿‡æ»¤æ— æ•ˆæ–‡æœ¬
    valid_queries = [q for q in query if q and isinstance(q, str) and q.strip()]
    if not valid_queries:
        return np.array([])

    try:
        # ğŸ‘‡ ç›´æ¥è°ƒç”¨å…¨å±€çš„æœ¬åœ°æ¨¡å‹å®ä¾‹
        # get_vec_batch ä¼šè‡ªåŠ¨å¤„ç† batchï¼Œå¹¶ä¸”å› ä¸ºæ˜¯æœ¬åœ°æ¨¡å‹ï¼Œæ²¡æœ‰ TPM é™åˆ¶
        print(f"ğŸš€ [æœ¬åœ°] æ­£åœ¨å‘é‡åŒ– {len(valid_queries)} æ¡æ–‡æœ¬...", flush=True)

        # text2vec è¿”å›çš„æ˜¯ Torch Tensorï¼Œéœ€è¦è½¬æˆ Numpy ç»™ Faiss ç”¨
        vectors = global_vector_model.get_vec_batch(valid_queries, bs=batch_size)

        if isinstance(vectors, torch.Tensor):
            return vectors.cpu().numpy()
        elif isinstance(vectors, list):
            return np.array(vectors)
        else:
            return np.array(vectors)

    except Exception as e:
        print(f"âŒ å‘é‡åŒ–å¤±è´¥: {str(e)}")
        traceback.print_exc()
        return np.array([])


# ç®€å•çš„å‘é‡æœç´¢ï¼Œç”¨äºåŸºæœ¬å¯¹æ¯”
def vector_search(query, index_path, metadata_path, limit):
    """åŸºæœ¬å‘é‡æœç´¢å‡½æ•°"""
    query_vector = vectorize_query(query)
    if query_vector.size == 0:
        return []

    query_vector = np.array(query_vector, dtype=np.float32).reshape(1, -1)

    index = faiss.read_index(index_path)
    try:
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
    except UnicodeDecodeError:
        print(f"è­¦å‘Šï¼š{metadata_path} åŒ…å«éæ³•å­—ç¬¦ï¼Œä½¿ç”¨ UTF-8 å¿½ç•¥é”™è¯¯é‡æ–°åŠ è½½")
        with open(metadata_path, 'rb') as f:
            content = f.read().decode('utf-8', errors='ignore')
            metadata = json.loads(content)

    D, I = index.search(query_vector, limit)
    results = [metadata[i] for i in I[0] if i < len(metadata)]
    return results


def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ä¸­çš„éæ³•å­—ç¬¦ï¼Œæ§åˆ¶æ–‡æœ¬é•¿åº¦"""
    if not text:
        return ""
    # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼Œä¿ç•™æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    # ç§»é™¤é‡å¤çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    # ç¡®ä¿æ–‡æœ¬é•¿åº¦åœ¨åˆç†èŒƒå›´å†…
    return text.strip()


# PDFæ–‡æœ¬æå–
def extract_text_from_pdf(pdf_path):
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            page_text = page.get_text()
            # æ¸…ç†ä¸å¯æ‰“å°å­—ç¬¦ï¼Œå°è¯•ç”¨ UTF-8 è§£ç ï¼Œå¤±è´¥æ—¶å¿½ç•¥éæ³•å­—ç¬¦
            text += page_text.encode('utf-8', errors='ignore').decode('utf-8')
        if not text.strip():
            print(f"è­¦å‘Šï¼šPDFæ–‡ä»¶ {pdf_path} æå–å†…å®¹ä¸ºç©º")
        return text
    except Exception as e:
        print(f"PDFæ–‡æœ¬æå–å¤±è´¥ï¼š{str(e)}")
        return ""


# å¤„ç†å•ä¸ªæ–‡ä»¶
def process_single_file(file_path: str) -> str:
    try:
        if file_path.lower().endswith('.pdf'):
            text = extract_text_from_pdf(file_path)
            if not text:
                return f"PDFæ–‡ä»¶ {file_path} å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–"
        else:
            with open(file_path, "rb") as f:
                content = f.read()
            result = chardet.detect(content)
            detected_encoding = result['encoding']
            confidence = result['confidence']

            # å°è¯•å¤šç§ç¼–ç æ–¹å¼
            if detected_encoding and confidence > 0.7:
                try:
                    text = content.decode(detected_encoding)
                    print(f"æ–‡ä»¶ {file_path} ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {detected_encoding} è§£ç æˆåŠŸ")
                except UnicodeDecodeError:
                    text = content.decode('utf-8', errors='ignore')
                    print(f"æ–‡ä»¶ {file_path} ä½¿ç”¨ {detected_encoding} è§£ç å¤±è´¥ï¼Œå¼ºåˆ¶ä½¿ç”¨ UTF-8 å¿½ç•¥éæ³•å­—ç¬¦")
            else:
                # å°è¯•å¤šç§å¸¸è§ç¼–ç 
                encodings = ['utf-8', 'gbk', 'gb18030', 'gb2312', 'latin-1', 'utf-16', 'cp936', 'big5']
                text = None
                for encoding in encodings:
                    try:
                        text = content.decode(encoding)
                        print(f"æ–‡ä»¶ {file_path} ä½¿ç”¨ {encoding} è§£ç æˆåŠŸ")
                        break
                    except UnicodeDecodeError:
                        continue

                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨å¿½ç•¥é”™è¯¯çš„æ–¹å¼è§£ç 
                if text is None:
                    text = content.decode('utf-8', errors='ignore')
                    print(f"è­¦å‘Šï¼šæ–‡ä»¶ {file_path} ä½¿ç”¨ UTF-8 å¿½ç•¥éæ³•å­—ç¬¦")

        # ç¡®ä¿æ–‡æœ¬æ˜¯å¹²å‡€çš„ï¼Œç§»é™¤éæ³•å­—ç¬¦
        text = clean_text(text)
        return text
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
        traceback.print_exc()
        return f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥ï¼š{str(e)}"


# æ‰¹é‡å¤„ç†å¹¶ç´¢å¼•æ–‡ä»¶ - ä¿®æ”¹ä¸ºæ”¯æŒæŒ‡å®šçŸ¥è¯†åº“
def process_and_index_files(file_objs: List, kb_name: str = "default_kb") -> str:
    print(f"ï¼ï¼ï¼è¿›å…¥æ–‡ä»¶å¤„ç†å‡½æ•° - ç›®æ ‡çŸ¥è¯†åº“: {kb_name}ï¼ï¼ï¼", flush=True)

    # 1. è·¯å¾„å‡†å¤‡
    kb_dir = os.path.join(KB_BASE_DIR, kb_name)
    os.makedirs(kb_dir, exist_ok=True)

    # ä¸´æ—¶æ–‡ä»¶å¤¹ä»…ç”¨äºæš‚å­˜ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶æµï¼Œç”¨å®Œå³åˆ 
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ğŸŸ¢ æ ¸å¿ƒä¿®æ­£ï¼šä¸­é—´æ•°æ®æ–‡ä»¶å¿…é¡»å­˜åœ¨å„è‡ªçš„çŸ¥è¯†åº“ç›®å½•ä¸‹ï¼Œå®ç°éš”ç¦»ï¼
    # è¿™æ · KB_A çš„æ•°æ®æ°¸è¿œä¸ä¼šè·‘åˆ° KB_B é‡Œå»
    semantic_chunk_output = os.path.join(kb_dir, "dataset_source.json")  # çº¯æ–‡æœ¬è´¦æœ¬
    semantic_chunk_vector = os.path.join(kb_dir, "dataset_vector.json")  # å‘é‡è´¦æœ¬

    # æœ€ç»ˆç´¢å¼•æ–‡ä»¶
    semantic_chunk_index = os.path.join(kb_dir, "semantic_chunk.index")
    semantic_chunk_metadata = os.path.join(kb_dir, "semantic_chunk_metadata.json")

    new_chunks_buffer = []
    error_messages = []

    try:
        if not file_objs: return "é”™è¯¯ï¼šæ²¡æœ‰é€‰æ‹©ä»»ä½•æ–‡ä»¶"
        print(f"å¼€å§‹å¤„ç† {len(file_objs)} ä¸ªæ–‡ä»¶...")

        for file_obj in file_objs:
            # è·å–è·¯å¾„å’Œæ–‡ä»¶å
            source_path = file_obj.name if hasattr(file_obj, 'name') else file_obj
            filename = os.path.basename(source_path)
            file_ext = os.path.splitext(filename)[1].lower()

            # æ„é€ ä¸´æ—¶è·¯å¾„ (é¿å… Windows é”)
            tmp_filename = f"proc_{hashlib.md5(filename.encode()).hexdigest()}{file_ext}"
            tmp_path = os.path.join(OUTPUT_DIR, tmp_filename)

            try:
                # å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                shutil.copy2(source_path, tmp_path)
                # å¤‡ä»½åŸå§‹æ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½• (ç•™æ¡£)
                shutil.copy2(tmp_path, os.path.join(kb_dir, filename))

                # è¯»å–æ–‡æœ¬
                raw_text = ""
                if file_ext == ".pdf":
                    try:
                        doc = fitz.open(tmp_path)
                        text_list = [page.get_text() for page in doc]
                        raw_text = "\n\n".join(text_list)
                        doc.close()
                    except Exception as e:
                        print(f"PDFè§£æå¤±è´¥: {e}")
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šLangChain Loader
                        try:
                            loader = PyMuPDFLoader(tmp_path)
                            pages = loader.load()
                            raw_text = "\n\n".join([p.page_content for p in pages])
                        except:
                            error_messages.append(f"{filename} è§£æå¤±è´¥")
                            continue
                else:
                    # æ–‡æœ¬/ä»£ç æ–‡ä»¶è¯»å–
                    with open(tmp_path, "rb") as f:
                        content_bytes = f.read()

                    decoded = False
                    for enc in ['utf-8', 'gbk', 'gb18030', 'latin-1']:
                        try:
                            raw_text = content_bytes.decode(enc)
                            decoded = True
                            break
                        except:
                            continue
                    if not decoded:
                        raw_text = content_bytes.decode('utf-8', errors='ignore')

                # åˆ†å—
                raw_text = clean_text(raw_text)
                if raw_text and len(raw_text) > 10:
                    chunks_str_list = semantic_chunk(raw_text, file_ext)

                    for chunk_text in chunks_str_list:
                        # ç”Ÿæˆå”¯ä¸€ID
                        unique_str = f"{filename}_{chunk_text}"
                        chunk_id = hashlib.md5(unique_str.encode('utf-8')).hexdigest()

                        new_chunks_buffer.append({
                            "id": chunk_id,
                            "chunk": chunk_text,
                            "metadata": {"source": filename},
                            "method": "semantic_chunk"
                        })
                else:
                    error_messages.append(f"æ–‡ä»¶ {filename} å†…å®¹ä¸ºç©º")

            except Exception as e:
                error_messages.append(f"æ–‡ä»¶ {filename} å¤„ç†å¼‚å¸¸: {str(e)}")
                traceback.print_exc()
            finally:
                if os.path.exists(tmp_path):
                    try:
                        os.remove(tmp_path)
                    except:
                        pass

        if not new_chunks_buffer:
            return "æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆåˆ†å—ã€‚\n" + "\n".join(error_messages)

        # 3. å¢é‡æ›´æ–° (è¯»å–å½“å‰çŸ¥è¯†åº“ä¸“å±çš„ JSON)
        final_all_chunks = []
        if os.path.exists(semantic_chunk_output):
            try:
                with open(semantic_chunk_output, 'r', encoding='utf-8') as f:
                    final_all_chunks = json.load(f)
            except:
                final_all_chunks = []

        # åˆå¹¶å»é‡
        chunk_map = {item["id"]: item for item in final_all_chunks}
        for item in new_chunks_buffer:
            chunk_map[item["id"]] = item
        final_all_chunks = list(chunk_map.values())

        # å†™å›å½“å‰çŸ¥è¯†åº“ç›®å½•
        with open(semantic_chunk_output, 'w', encoding='utf-8') as f:
            json.dump(final_all_chunks, f, ensure_ascii=False, indent=4)

        print(f"æ•°æ®åˆå¹¶å®Œæˆã€‚å½“å‰çŸ¥è¯†åº“ {kb_name} æ€»æ¡ç›®: {len(final_all_chunks)}")

        # 4. å‘é‡åŒ–
        print("å¼€å§‹å…¨é‡å‘é‡åŒ–...")
        vectorize_file(final_all_chunks, semantic_chunk_vector)

        # 5. æ„å»ºç´¢å¼•
        print("å¼€å§‹æ„å»ºç´¢å¼•...")
        build_faiss_index(semantic_chunk_vector, semantic_chunk_index, semantic_chunk_metadata)

        status_msg = f"æˆåŠŸï¼çŸ¥è¯†åº“ '{kb_name}' æ›´æ–°å®Œæ¯•ã€‚\næ€»æ¡ç›®: {len(final_all_chunks)}\næœ¬æ¬¡æ–°å¢: {len(new_chunks_buffer)}"
        if error_messages:
            status_msg += "\n\nâš ï¸ éƒ¨åˆ†æ–‡ä»¶è­¦å‘Š:\n" + "\n".join(error_messages)
        return status_msg

    except Exception as e:
        traceback.print_exc()
        return f"ä¸¥é‡é”™è¯¯: {str(e)}"


# æ ¸å¿ƒè”ç½‘æœç´¢åŠŸèƒ½
def get_search_background(query: str, max_length: int = 1500) -> str:
    try:
        from retrievor import q_searching
        search_results = q_searching(query)
        cleaned_results = re.sub(r'\s+', ' ', search_results).strip()
        return cleaned_results[:max_length]
    except Exception as e:
        print(f"è”ç½‘æœç´¢å¤±è´¥ï¼š{str(e)}")
        return ""


# åŸºæœ¬çš„å›ç­”ç”Ÿæˆ
def generate_answer_from_deepseek(question: str, system_prompt: str = "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ FPGA/ASIC å·¥ç¨‹å¸ˆï¼Œç²¾é€š Verilog å’Œ SystemVerilogï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®èƒŒæ™¯çŸ¥è¯†ï¼Œç¼–å†™é«˜è´¨é‡ã€å¯ç»¼åˆçš„ Verilog ä»£ç ã€‚",
                                  background_info: Optional[str] = None) -> str:
    deepseek_client = DeepSeekClient()
    user_prompt = f"é—®é¢˜ï¼š{question}"
    if background_info:
        user_prompt = f"èƒŒæ™¯çŸ¥è¯†ï¼š{background_info}\n\n{user_prompt}"
    try:
        answer = deepseek_client.generate_answer(system_prompt, user_prompt)
        return answer
    except Exception as e:
        return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼š{str(e)}"


# å¤šè·³æ¨ç†RAGç³»ç»Ÿ - æ ¸å¿ƒåˆ›æ–°ç‚¹
class ReasoningRAG:
    """
    å¤šè·³æ¨ç†RAGç³»ç»Ÿï¼Œé€šè¿‡è¿­ä»£å¼çš„æ£€ç´¢å’Œæ¨ç†è¿‡ç¨‹å›ç­”é—®é¢˜ï¼Œæ”¯æŒæµå¼å“åº”
    """

    def __init__(self,
                 index_path: str,
                 metadata_path: str,
                 max_hops: int = 3,
                 initial_candidates: int = 5,
                 refined_candidates: int = 3,
                 reasoning_model: str = Config.llm_model,
                 verbose: bool = False):
        """
        åˆå§‹åŒ–æ¨ç†RAGç³»ç»Ÿ

        å‚æ•°:
            index_path: FAISSç´¢å¼•çš„è·¯å¾„
            metadata_path: å…ƒæ•°æ®JSONæ–‡ä»¶çš„è·¯å¾„
            max_hops: æœ€å¤§æ¨ç†-æ£€ç´¢è·³æ•°
            initial_candidates: åˆå§‹æ£€ç´¢å€™é€‰æ•°é‡
            refined_candidates: ç²¾ç‚¼æ£€ç´¢å€™é€‰æ•°é‡
            reasoning_model: ç”¨äºæ¨ç†æ­¥éª¤çš„LLMæ¨¡å‹
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        """
        self.index_path = index_path
        self.metadata_path = metadata_path
        self.max_hops = max_hops
        self.initial_candidates = initial_candidates
        self.refined_candidates = refined_candidates
        self.reasoning_model = reasoning_model
        self.verbose = verbose

        # åŠ è½½ç´¢å¼•å’Œå…ƒæ•°æ®
        self._load_resources()

    def _load_resources(self):
        """åŠ è½½FAISSç´¢å¼•å’Œå…ƒæ•°æ®"""
        if os.path.exists(self.index_path) and os.path.exists(self.metadata_path):
            self.index = faiss.read_index(self.index_path)
            try:
                with open(self.metadata_path, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
            except UnicodeDecodeError:
                with open(self.metadata_path, 'rb') as f:
                    content = f.read().decode('utf-8', errors='ignore')
                    self.metadata = json.loads(content)
        else:
            raise FileNotFoundError(f"Index or metadata not found at {self.index_path} or {self.metadata_path}")

    def _vectorize_query(self, query: str) -> np.ndarray:
        """å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡"""
        return vectorize_query(query).reshape(1, -1)

    def _retrieve(self, query_vector: np.ndarray, limit: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å‘é‡ç›¸ä¼¼æ€§æ£€ç´¢å—"""
        if query_vector.size == 0:
            return []

        D, I = self.index.search(query_vector, limit)
        results = [self.metadata[i] for i in I[0] if i < len(self.metadata)]
        return results

    def _generate_reasoning(self,
                            query: str,
                            retrieved_chunks: List[Dict[str, Any]],
                            previous_queries: List[str] = None,
                            hop_number: int = 0) -> Dict[str, Any]:
        """
        ä¸ºæ£€ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆæ¨ç†åˆ†æå¹¶è¯†åˆ«ä¿¡æ¯ç¼ºå£

        è¿”å›åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
            - analysis: å¯¹å½“å‰ä¿¡æ¯çš„æ¨ç†åˆ†æ
            - missing_info: å·²è¯†åˆ«çš„ç¼ºå¤±ä¿¡æ¯
            - follow_up_queries: å¡«è¡¥ä¿¡æ¯ç¼ºå£çš„åç»­æŸ¥è¯¢åˆ—è¡¨
            - is_sufficient: è¡¨ç¤ºä¿¡æ¯æ˜¯å¦è¶³å¤Ÿçš„å¸ƒå°”å€¼
        """
        if previous_queries is None:
            previous_queries = []

        # ä¸ºæ¨¡å‹å‡†å¤‡ä¸Šä¸‹æ–‡
        chunks_text = "\n\n".join([f"[Chunk {i + 1}]: {chunk['chunk']}"
                                   for i, chunk in enumerate(retrieved_chunks)])

        previous_queries_text = "\n".join([f"Q{i + 1}: {q}" for i, q in enumerate(previous_queries)])

        system_prompt = """
        ä½ æ˜¯ Verilog ä»£ç åº“çš„ä¸“å®¶åˆ†æç³»ç»Ÿã€‚
        ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææ£€ç´¢åˆ°çš„ä»£ç ç‰‡æ®µï¼Œè¯†åˆ«é€»è¾‘ç¼ºå£ï¼ˆå¦‚ç¼ºå¤±çš„å­æ¨¡å—ã€æœªå®šä¹‰çš„å‚æ•°ï¼‰ï¼Œå¹¶æå‡ºåç»­æŸ¥è¯¢ä»¥è¡¥å…¨ä»£ç ä¸Šä¸‹æ–‡ã€‚
        """

        user_prompt = f"""
        ## åŸå§‹æŸ¥è¯¢
        {query}

        ## å…ˆå‰æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰ï¼‰
        {previous_queries_text if previous_queries else "æ— "}

        ## æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼ˆè·³æ•° {hop_number}ï¼‰
        {chunks_text if chunks_text else "æœªæ£€ç´¢åˆ°ä¿¡æ¯ã€‚"}

        ## ä½ çš„ä»»åŠ¡
        1. åˆ†æå·²æ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸åŸå§‹æŸ¥è¯¢çš„å…³ç³»
        2. ç¡®å®šèƒ½å¤Ÿæ›´å®Œæ•´å›ç­”æŸ¥è¯¢çš„ç‰¹å®šç¼ºå¤±ä¿¡æ¯
        3. æå‡º1-3ä¸ªé’ˆå¯¹æ€§çš„åç»­æŸ¥è¯¢ï¼Œä»¥æ£€ç´¢ç¼ºå¤±ä¿¡æ¯
        4. ç¡®å®šå½“å‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿå›ç­”åŸå§‹æŸ¥è¯¢

        ä»¥JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:
        - analysis: å¯¹å½“å‰ä¿¡æ¯çš„è¯¦ç»†åˆ†æ
        - missing_info: ç‰¹å®šç¼ºå¤±ä¿¡æ¯çš„åˆ—è¡¨
        - follow_up_queries: 1-3ä¸ªå…·ä½“çš„åç»­æŸ¥è¯¢
        - is_sufficient: è¡¨ç¤ºä¿¡æ¯æ˜¯å¦è¶³å¤Ÿçš„å¸ƒå°”å€¼
        
        âš ï¸ **é‡è¦é€»è¾‘è§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
        1. **å¦‚æœä½ åœ¨ "missing_info" å­—æ®µä¸­åˆ—å‡ºäº†ä»»ä½•å†…å®¹ï¼Œé‚£ä¹ˆ "is_sufficient" å¿…é¡»ä¸º falseï¼**
        2. åªæœ‰å½“ä½ å®Œå…¨ç¡®å®šå½“å‰æ£€ç´¢åˆ°çš„ Chunk å·²ç»åŒ…å«äº†ç”¨æˆ·é—®é¢˜æ‰€éœ€çš„æ‰€æœ‰ç»†èŠ‚ï¼ˆæ— éœ€ä»»ä½•é¢å¤–æŸ¥è¯¢ï¼‰æ—¶ï¼Œ"is_sufficient" æ‰èƒ½ä¸º trueã€‚
        3. å¦‚æœä»£ç è¢«æˆªæ–­ï¼ˆChunkingï¼‰å¯¼è‡´å…³é”®éƒ¨åˆ†ï¼ˆå¦‚è¡Œå·ã€ä¸­é—´é€»è¾‘ï¼‰ä¸¢å¤±ï¼Œå¿…é¡»æ ‡è®°ä¸ºä¸å®Œæ•´ã€‚
        """

        try:
            response = client.chat.completions.create(
                model=Config.llm_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"}
            )
            reasoning_text = response.choices[0].message.content.strip()

            # è§£æJSONå“åº”
            try:
                reasoning = json.loads(reasoning_text)
                # ç¡®ä¿é¢„æœŸçš„é”®å­˜åœ¨
                required_keys = ["analysis", "missing_info", "follow_up_queries", "is_sufficient"]
                for key in required_keys:
                    if key not in reasoning:
                        reasoning[key] = [] if key != "is_sufficient" else False
                return reasoning
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ™å›é€€
                if self.verbose:
                    print(f"æ— æ³•ä»æ¨¡å‹è¾“å‡ºè§£æJSON: {reasoning_text[:100]}...")
                return {
                    "analysis": "æ— æ³•åˆ†ææ£€ç´¢åˆ°çš„ä¿¡æ¯ã€‚",
                    "missing_info": ["æ— æ³•è¯†åˆ«ç¼ºå¤±ä¿¡æ¯"],
                    "follow_up_queries": [],
                    "is_sufficient": False
                }

        except Exception as e:
            if self.verbose:
                print(f"æ¨ç†ç”Ÿæˆé”™è¯¯: {e}")
                print(traceback.format_exc())
            return {
                "analysis": "åˆ†æè¿‡ç¨‹å‡ºé”™ã€‚",
                "missing_info": [],
                "follow_up_queries": [],
                "is_sufficient": False
            }

    def _synthesize_answer(self,
                           query: str,
                           all_chunks: List[Dict[str, Any]],
                           reasoning_steps: List[Dict[str, Any]],
                           use_table_format: bool = False) -> str:
        """ä»æ‰€æœ‰æ£€ç´¢åˆ°çš„å—å’Œæ¨ç†æ­¥éª¤ä¸­åˆæˆæœ€ç»ˆç­”æ¡ˆ"""
        # åˆå¹¶æ‰€æœ‰å—ï¼Œå»é™¤é‡å¤
        unique_chunks = []
        chunk_ids = set()
        for chunk in all_chunks:
            if chunk["id"] not in chunk_ids:
                unique_chunks.append(chunk)
                chunk_ids.add(chunk["id"])

        # å‡†å¤‡ä¸Šä¸‹æ–‡
        chunks_text = "\n\n".join([f"[Chunk {i + 1}]: {chunk['chunk']}"
                                   for i, chunk in enumerate(unique_chunks)])

        # å‡†å¤‡æ¨ç†è·Ÿè¸ª
        reasoning_trace = ""
        for i, step in enumerate(reasoning_steps):
            reasoning_trace += f"\n\næ¨ç†æ­¥éª¤ {i + 1}:\n"
            reasoning_trace += f"åˆ†æ: {step['analysis']}\n"
            reasoning_trace += f"ç¼ºå¤±ä¿¡æ¯: {', '.join(step['missing_info'])}\n"
            reasoning_trace += f"åç»­æŸ¥è¯¢: {', '.join(step['follow_up_queries'])}"

        system_prompt = """
        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ FPGA/ASIC å·¥ç¨‹å¸ˆï¼Œç²¾é€š Verilog å’Œ SystemVerilogã€‚
        ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œæä¾›çš„å‚è€ƒä»£ç ï¼ˆContextï¼‰ï¼Œç¼–å†™é«˜è´¨é‡ã€å¯ç»¼åˆçš„ Verilog ä»£ç ã€‚

        è¦æ±‚ï¼š
        1. **ä»£ç ä¼˜å…ˆ**ï¼šç›´æ¥è¾“å‡ºä»£ç ï¼Œé™¤éç”¨æˆ·è¦æ±‚è§£é‡Šã€‚
        2. **è§„èŒƒæ€§**ï¼šå¿…é¡»åŒ…å« module å®šä¹‰ã€è¾“å…¥è¾“å‡ºç«¯å£ã€parameter å®šä¹‰ã€‚
        3. **æ—¶åºé€»è¾‘**ï¼šå¤„ç†å¤ä½é€»è¾‘ï¼ˆé€šå¸¸æ˜¯å¼‚æ­¥ä½ç”µå¹³å¤ä½æˆ–åŒæ­¥é«˜ç”µå¹³å¤ä½ï¼‰ã€‚
        4. **é£æ ¼ä¸€è‡´**ï¼šå‚è€ƒ Context ä¸­çš„å‘½åè§„èŒƒï¼ˆå¦‚ _i, _o, _rï¼‰å’Œç¼©è¿›é£æ ¼ã€‚
        5. **æ³¨é‡Š**ï¼šå…³é”®é€»è¾‘éœ€è¦æ·»åŠ ç®€çŸ­æ³¨é‡Šã€‚
        """

        output_format_instruction = ""
        if use_table_format:
            output_format_instruction = """
            è¯·å°½å¯èƒ½ä»¥Markdownè¡¨æ ¼æ ¼å¼ç»„ç»‡ä½ çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯é€‚åˆè¡¨æ ¼å½¢å¼å±•ç¤ºï¼Œè¯·ä½¿ç”¨è¡¨æ ¼ï¼›
            å¦‚æœä¸é€‚åˆè¡¨æ ¼å½¢å¼ï¼Œå¯ä»¥å…ˆç”¨æ–‡æœ¬ä»‹ç»ï¼Œç„¶åå†ä½¿ç”¨è¡¨æ ¼æ€»ç»“å…³é”®ä¿¡æ¯ã€‚

            è¡¨æ ¼è¯­æ³•ç¤ºä¾‹ï¼š
            | æ ‡é¢˜1 | æ ‡é¢˜2 | æ ‡é¢˜3 |
            | ----- | ----- | ----- |
            | å†…å®¹1 | å†…å®¹2 | å†…å®¹3 |

            ç¡®ä¿è¡¨æ ¼æ ¼å¼ç¬¦åˆMarkdownæ ‡å‡†ï¼Œä»¥ä¾¿æ­£ç¡®æ¸²æŸ“ã€‚
            """

        user_prompt = f"""
        ## åŸå§‹æŸ¥è¯¢
        {query}

        ## æ£€ç´¢åˆ°çš„ä¿¡æ¯å—
        {chunks_text}

        ## æ¨ç†è¿‡ç¨‹
        {reasoning_trace}

        ## ä½ çš„ä»»åŠ¡
        ä½¿ç”¨æä¾›çš„ä¿¡æ¯å—ä¸ºåŸå§‹æŸ¥è¯¢åˆæˆä¸€ä¸ªå…¨é¢çš„ç­”æ¡ˆã€‚ä½ çš„ç­”æ¡ˆåº”è¯¥:

        1. ç›´æ¥å›åº”æŸ¥è¯¢
        2. ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£
        3. åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯
        4. æ‰¿è®¤å¯ç”¨ä¿¡æ¯ä¸­çš„ä»»ä½•é‡å¤§ç¼ºå£

        {output_format_instruction}

        ä»¥ç›´æ¥å›åº”æå‡ºåŸå§‹æŸ¥è¯¢çš„ç”¨æˆ·çš„æ–¹å¼å‘ˆç°ä½ çš„ç­”æ¡ˆã€‚
        """

        try:
            response = client.chat.completions.create(
                model=Config.llm_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            if self.verbose:
                print(f"ç­”æ¡ˆåˆæˆé”™è¯¯: {e}")
                print(traceback.format_exc())
            return "ç”±äºå‡ºé”™ï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆã€‚"

    def stream_retrieve_and_answer(self, query: str, use_table_format: bool = False):
        """
        æ‰§è¡Œå¤šè·³æ£€ç´¢å’Œå›ç­”ç”Ÿæˆçš„æµå¼æ–¹æ³•ï¼Œé€æ­¥è¿”å›ç»“æœ

        è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°ï¼Œä¼šåœ¨å¤„ç†çš„æ¯ä¸ªé˜¶æ®µäº§ç”Ÿä¸­é—´ç»“æœ
        """
        all_chunks = []
        all_queries = [query]
        reasoning_steps = []

        # ç”ŸæˆçŠ¶æ€æ›´æ–°
        yield {
            "status": "æ­£åœ¨å°†æŸ¥è¯¢å‘é‡åŒ–...",
            "reasoning_display": "",
            "answer": None,
            "all_chunks": [],
            "reasoning_steps": []
        }

        # åˆå§‹æ£€ç´¢
        try:
            query_vector = self._vectorize_query(query)
            if query_vector.size == 0:
                yield {
                    "status": "å‘é‡åŒ–å¤±è´¥",
                    "reasoning_display": "ç”±äºåµŒå…¥é”™è¯¯ï¼Œæ— æ³•å¤„ç†æŸ¥è¯¢ã€‚",
                    "answer": "ç”±äºåµŒå…¥é”™è¯¯ï¼Œæ— æ³•å¤„ç†æŸ¥è¯¢ã€‚",
                    "all_chunks": [],
                    "reasoning_steps": []
                }
                return

            yield {
                "status": "æ­£åœ¨æ‰§è¡Œåˆå§‹æ£€ç´¢...",
                "reasoning_display": "",
                "answer": None,
                "all_chunks": [],
                "reasoning_steps": []
            }

            initial_chunks = self._retrieve(query_vector, self.initial_candidates)
            all_chunks.extend(initial_chunks)

            if not initial_chunks:
                yield {
                    "status": "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯",
                    "reasoning_display": "æœªæ‰¾åˆ°ä¸æ‚¨çš„æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ã€‚",
                    "answer": "æœªæ‰¾åˆ°ä¸æ‚¨çš„æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ã€‚",
                    "all_chunks": [],
                    "reasoning_steps": []
                }
                return

            # æ›´æ–°çŠ¶æ€ï¼Œå±•ç¤ºæ‰¾åˆ°çš„åˆå§‹å—
            chunks_preview = "\n".join([f"- {chunk['chunk'][:100]}..." for chunk in initial_chunks[:2]])
            yield {
                "status": f"æ‰¾åˆ° {len(initial_chunks)} ä¸ªç›¸å…³ä¿¡æ¯å—ï¼Œæ­£åœ¨ç”Ÿæˆåˆæ­¥åˆ†æ...",
                "reasoning_display": f"### æ£€ç´¢åˆ°çš„åˆå§‹ä¿¡æ¯\n{chunks_preview}\n\n### æ­£åœ¨åˆ†æ...",
                "answer": None,
                "all_chunks": all_chunks,
                "reasoning_steps": []
            }

            # åˆå§‹æ¨ç†
            reasoning = self._generate_reasoning(query, initial_chunks, hop_number=0)
            reasoning_steps.append(reasoning)

            # ç”Ÿæˆå½“å‰çš„æ¨ç†æ˜¾ç¤º
            reasoning_display = "### å¤šè·³æ¨ç†è¿‡ç¨‹\n"
            reasoning_display += f"**æ¨ç†æ­¥éª¤ 1**\n"
            reasoning_display += f"- åˆ†æ: {reasoning['analysis'][:200]}...\n"
            reasoning_display += f"- ç¼ºå¤±ä¿¡æ¯: {', '.join(reasoning['missing_info'])}\n"
            if reasoning['follow_up_queries']:
                reasoning_display += f"- åç»­æŸ¥è¯¢: {', '.join(reasoning['follow_up_queries'])}\n"
            reasoning_display += f"- ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿ: {'æ˜¯' if reasoning['is_sufficient'] else 'å¦'}\n\n"

            yield {
                "status": "åˆæ­¥åˆ†æå®Œæˆ",
                "reasoning_display": reasoning_display,
                "answer": None,
                "all_chunks": all_chunks,
                "reasoning_steps": reasoning_steps
            }

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢å¤–çš„è·³æ•°
            hop = 1
            while (hop < self.max_hops and
                   not reasoning["is_sufficient"] and
                   reasoning["follow_up_queries"]):

                follow_up_status = f"æ‰§è¡Œè·³æ•° {hop}ï¼Œæ­£åœ¨å¤„ç† {len(reasoning['follow_up_queries'])} ä¸ªåç»­æŸ¥è¯¢..."
                yield {
                    "status": follow_up_status,
                    "reasoning_display": reasoning_display + f"\n\n### {follow_up_status}",
                    "answer": None,
                    "all_chunks": all_chunks,
                    "reasoning_steps": reasoning_steps
                }

                hop_chunks = []

                # å¤„ç†æ¯ä¸ªåç»­æŸ¥è¯¢
                for i, follow_up_query in enumerate(reasoning["follow_up_queries"]):
                    all_queries.append(follow_up_query)

                    query_status = f"å¤„ç†åç»­æŸ¥è¯¢ {i + 1}/{len(reasoning['follow_up_queries'])}: {follow_up_query}"
                    yield {
                        "status": query_status,
                        "reasoning_display": reasoning_display + f"\n\n### {query_status}",
                        "answer": None,
                        "all_chunks": all_chunks,
                        "reasoning_steps": reasoning_steps
                    }

                    # ä¸ºåç»­æŸ¥è¯¢æ£€ç´¢
                    follow_up_vector = self._vectorize_query(follow_up_query)
                    if follow_up_vector.size > 0:
                        follow_up_chunks = self._retrieve(follow_up_vector, self.refined_candidates)
                        hop_chunks.extend(follow_up_chunks)
                        all_chunks.extend(follow_up_chunks)

                        # æ›´æ–°çŠ¶æ€ï¼Œæ˜¾ç¤ºæ–°æ‰¾åˆ°çš„å—æ•°é‡
                        yield {
                            "status": f"æŸ¥è¯¢ '{follow_up_query}' æ‰¾åˆ°äº† {len(follow_up_chunks)} ä¸ªç›¸å…³å—",
                            "reasoning_display": reasoning_display + f"\n\nä¸ºæŸ¥è¯¢ '{follow_up_query}' æ‰¾åˆ°äº† {len(follow_up_chunks)} ä¸ªç›¸å…³å—",
                            "answer": None,
                            "all_chunks": all_chunks,
                            "reasoning_steps": reasoning_steps
                        }

                # ä¸ºæ­¤è·³æ•°ç”Ÿæˆæ¨ç†
                yield {
                    "status": f"æ­£åœ¨ä¸ºè·³æ•° {hop} ç”Ÿæˆæ¨ç†åˆ†æ...",
                    "reasoning_display": reasoning_display + f"\n\n### æ­£åœ¨ä¸ºè·³æ•° {hop} ç”Ÿæˆæ¨ç†åˆ†æ...",
                    "answer": None,
                    "all_chunks": all_chunks,
                    "reasoning_steps": reasoning_steps
                }

                reasoning = self._generate_reasoning(
                    query,
                    hop_chunks,
                    previous_queries=all_queries[:-1],
                    hop_number=hop
                )
                reasoning_steps.append(reasoning)

                # æ›´æ–°æ¨ç†æ˜¾ç¤º
                reasoning_display += f"\n**æ¨ç†æ­¥éª¤ {hop + 1}**\n"
                reasoning_display += f"- åˆ†æ: {reasoning['analysis'][:200]}...\n"
                reasoning_display += f"- ç¼ºå¤±ä¿¡æ¯: {', '.join(reasoning['missing_info'])}\n"
                if reasoning['follow_up_queries']:
                    reasoning_display += f"- åç»­æŸ¥è¯¢: {', '.join(reasoning['follow_up_queries'])}\n"
                reasoning_display += f"- ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿ: {'æ˜¯' if reasoning['is_sufficient'] else 'å¦'}\n"

                yield {
                    "status": f"è·³æ•° {hop} å®Œæˆ",
                    "reasoning_display": reasoning_display,
                    "answer": None,
                    "all_chunks": all_chunks,
                    "reasoning_steps": reasoning_steps
                }

                hop += 1

            # åˆæˆæœ€ç»ˆç­”æ¡ˆ
            yield {
                "status": "æ­£åœ¨åˆæˆæœ€ç»ˆç­”æ¡ˆ...",
                "reasoning_display": reasoning_display + "\n\n### æ­£åœ¨åˆæˆæœ€ç»ˆç­”æ¡ˆ...",
                "answer": "æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨å€™...",
                "all_chunks": all_chunks,
                "reasoning_steps": reasoning_steps
            }

            answer = self._synthesize_answer(query, all_chunks, reasoning_steps, use_table_format)

            # ä¸ºæœ€ç»ˆæ˜¾ç¤ºå‡†å¤‡æ£€ç´¢å†…å®¹æ±‡æ€»
            all_chunks_summary = "\n\n".join([f"**æ£€ç´¢å— {i + 1}**:\n{chunk['chunk']}"
                                              for i, chunk in enumerate(all_chunks[:10])])  # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ªå—

            if len(all_chunks) > 10:
                all_chunks_summary += f"\n\n...ä»¥åŠå¦å¤– {len(all_chunks) - 10} ä¸ªå—ï¼ˆæ€»è®¡ {len(all_chunks)} ä¸ªï¼‰"

            enhanced_display = reasoning_display + "\n\n### æ£€ç´¢åˆ°çš„å†…å®¹\n" + all_chunks_summary + "\n\n### å›ç­”å·²ç”Ÿæˆ"

            yield {
                "status": "å›ç­”å·²ç”Ÿæˆ",
                "reasoning_display": enhanced_display,
                "answer": answer,
                "all_chunks": all_chunks,
                "reasoning_steps": reasoning_steps
            }

        except Exception as e:
            error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
            if self.verbose:
                print(error_msg)
                print(traceback.format_exc())

            yield {
                "status": "å¤„ç†å‡ºé”™",
                "reasoning_display": error_msg,
                "answer": f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                "all_chunks": all_chunks,
                "reasoning_steps": reasoning_steps
            }

    def retrieve_and_answer(self, query: str, use_table_format: bool = False) -> Tuple[str, Dict[str, Any]]:
        """
        æ‰§è¡Œå¤šè·³æ£€ç´¢å’Œå›ç­”ç”Ÿæˆçš„ä¸»è¦æ–¹æ³•

        è¿”å›:
            åŒ…å«ä»¥ä¸‹å†…å®¹çš„å…ƒç»„:
            - æœ€ç»ˆç­”æ¡ˆ
            - åŒ…å«æ¨ç†æ­¥éª¤å’Œæ‰€æœ‰æ£€ç´¢åˆ°çš„å—çš„è°ƒè¯•å­—å…¸
        """
        all_chunks = []
        all_queries = [query]
        reasoning_steps = []
        debug_info = {"reasoning_steps": [], "all_chunks": [], "all_queries": all_queries}

        # åˆå§‹æ£€ç´¢
        query_vector = self._vectorize_query(query)
        if query_vector.size == 0:
            return "ç”±äºåµŒå…¥é”™è¯¯ï¼Œæ— æ³•å¤„ç†æŸ¥è¯¢ã€‚", debug_info

        initial_chunks = self._retrieve(query_vector, self.initial_candidates)
        all_chunks.extend(initial_chunks)
        debug_info["all_chunks"].extend(initial_chunks)

        if not initial_chunks:
            return "æœªæ‰¾åˆ°ä¸æ‚¨çš„æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ã€‚", debug_info

        # åˆå§‹æ¨ç†
        reasoning = self._generate_reasoning(query, initial_chunks, hop_number=0)
        reasoning_steps.append(reasoning)
        debug_info["reasoning_steps"].append(reasoning)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢å¤–çš„è·³æ•°
        hop = 1
        while (hop < self.max_hops and
               not reasoning["is_sufficient"] and
               reasoning["follow_up_queries"]):

            if self.verbose:
                print(f"å¼€å§‹è·³æ•° {hop}ï¼Œæœ‰ {len(reasoning['follow_up_queries'])} ä¸ªåç»­æŸ¥è¯¢")

            hop_chunks = []

            # å¤„ç†æ¯ä¸ªåç»­æŸ¥è¯¢
            for follow_up_query in reasoning["follow_up_queries"]:
                all_queries.append(follow_up_query)
                debug_info["all_queries"].append(follow_up_query)

                # ä¸ºåç»­æŸ¥è¯¢æ£€ç´¢
                follow_up_vector = self._vectorize_query(follow_up_query)
                if follow_up_vector.size > 0:
                    follow_up_chunks = self._retrieve(follow_up_vector, self.refined_candidates)
                    hop_chunks.extend(follow_up_chunks)
                    all_chunks.extend(follow_up_chunks)
                    debug_info["all_chunks"].extend(follow_up_chunks)

            # ä¸ºæ­¤è·³æ•°ç”Ÿæˆæ¨ç†
            reasoning = self._generate_reasoning(
                query,
                hop_chunks,
                previous_queries=all_queries[:-1],
                hop_number=hop
            )
            reasoning_steps.append(reasoning)
            debug_info["reasoning_steps"].append(reasoning)

            hop += 1

        # åˆæˆæœ€ç»ˆç­”æ¡ˆ
        answer = self._synthesize_answer(query, all_chunks, reasoning_steps, use_table_format)

        return answer, debug_info


# åŸºäºé€‰å®šçŸ¥è¯†åº“ç”Ÿæˆç´¢å¼•è·¯å¾„
def get_kb_paths(kb_name: str) -> Dict[str, str]:
    """è·å–æŒ‡å®šçŸ¥è¯†åº“çš„ç´¢å¼•æ–‡ä»¶è·¯å¾„"""
    kb_dir = os.path.join(KB_BASE_DIR, kb_name)
    return {
        "index_path": os.path.join(kb_dir, "semantic_chunk.index"),
        "metadata_path": os.path.join(kb_dir, "semantic_chunk_metadata.json")
    }


def multi_hop_generate_answer(query: str, kb_name: str, use_table_format: bool = False,
                              system_prompt: str = "ä½ æ˜¯ä¸€åverilogä¸“å®¶ã€‚") -> Tuple[str, Dict]:
    """ä½¿ç”¨å¤šè·³æ¨ç†RAGç”Ÿæˆç­”æ¡ˆï¼ŒåŸºäºæŒ‡å®šçŸ¥è¯†åº“"""
    kb_paths = get_kb_paths(kb_name)

    reasoning_rag = ReasoningRAG(
        index_path=kb_paths["index_path"],
        metadata_path=kb_paths["metadata_path"],
        max_hops=3,
        initial_candidates=5,
        refined_candidates=3,
        reasoning_model=Config.llm_model,
        verbose=True
    )

    answer, debug_info = reasoning_rag.retrieve_and_answer(query, use_table_format)
    return answer, debug_info


# ä½¿ç”¨ç®€å•å‘é‡æ£€ç´¢ç”Ÿæˆç­”æ¡ˆï¼ŒåŸºäºæŒ‡å®šçŸ¥è¯†åº“
def simple_generate_answer(query: str, kb_name: str, use_table_format: bool = False) -> str:
    """ä½¿ç”¨ç®€å•çš„å‘é‡æ£€ç´¢ç”Ÿæˆç­”æ¡ˆï¼Œä¸ä½¿ç”¨å¤šè·³æ¨ç†"""
    try:
        kb_paths = get_kb_paths(kb_name)

        # ä½¿ç”¨åŸºæœ¬å‘é‡æœç´¢
        search_results = vector_search(query, kb_paths["index_path"], kb_paths["metadata_path"], limit=5)

        if not search_results:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # å‡†å¤‡èƒŒæ™¯ä¿¡æ¯
        background_chunks = "\n\n".join([
            f"// --- [Reference Code {i + 1}] ---\n{result['chunk']}\n// ---------------------------"
            for i, result in enumerate(search_results)
        ])

        # ç”Ÿæˆç­”æ¡ˆ
        system_prompt = VERILOG_SYSTEM_PROMPT

        if use_table_format:
            system_prompt += "è¯·å°½å¯èƒ½ä»¥Markdownè¡¨æ ¼çš„å½¢å¼å‘ˆç°ç»“æ„åŒ–ä¿¡æ¯ã€‚"

        user_prompt = f"""
        é—®é¢˜ï¼š{query}

        èƒŒæ™¯ä¿¡æ¯ï¼š
        {background_chunks}

        è¯·åŸºäºä»¥ä¸ŠèƒŒæ™¯ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        """

        response = client.chat.completions.create(
            model=Config.llm_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )

        return response.choices[0].message.content.strip()

    except Exception as e:
        return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼š{str(e)}"


# ä¿®æ”¹ä¸»è¦çš„é—®é¢˜å¤„ç†å‡½æ•°ä»¥æ”¯æŒæŒ‡å®šçŸ¥è¯†åº“
def ask_question_parallel(question: str, kb_name: str = DEFAULT_KB, use_search: bool = True,
                          use_table_format: bool = False, multi_hop: bool = False) -> str:
    """åŸºäºæŒ‡å®šçŸ¥è¯†åº“å›ç­”é—®é¢˜"""
    try:
        kb_paths = get_kb_paths(kb_name)
        index_path = kb_paths["index_path"]
        metadata_path = kb_paths["metadata_path"]

        search_background = ""
        local_answer = ""
        debug_info = {}

        # å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = {}

            if use_search:
                futures[executor.submit(get_search_background, question)] = "search"

            if os.path.exists(index_path):
                if multi_hop:
                    # ä½¿ç”¨å¤šè·³æ¨ç†
                    futures[executor.submit(multi_hop_generate_answer, question, kb_name, use_table_format)] = "rag"
                else:
                    # ä½¿ç”¨ç®€å•å‘é‡æ£€ç´¢
                    futures[executor.submit(simple_generate_answer, question, kb_name, use_table_format)] = "simple"

            for future in as_completed(futures):
                result = future.result()
                if futures[future] == "search":
                    search_background = result or ""
                elif futures[future] == "rag":
                    local_answer, debug_info = result
                elif futures[future] == "simple":
                    local_answer = result

        # å¦‚æœåŒæ—¶æœ‰æœç´¢å’Œæœ¬åœ°ç»“æœï¼Œåˆå¹¶å®ƒä»¬
        if search_background and local_answer:
            system_prompt = "ä½ æ˜¯ä¸€åç¡¬ä»¶å·¥ç¨‹å¸ˆï¼Œè¯·æ•´åˆç½‘ç»œèµ„æ–™å’Œæœ¬åœ°ä»£ç åº“ï¼Œç¼–å†™ Verilog æ¨¡å—ã€‚"

            table_instruction = ""
            if use_table_format:
                table_instruction = """
                è¯·å°½å¯èƒ½ä»¥Markdownè¡¨æ ¼çš„å½¢å¼å‘ˆç°ä½ çš„å›ç­”ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç—‡çŠ¶ã€æ²»ç–—æ–¹æ³•ã€è¯ç‰©ç­‰ç»“æ„åŒ–ä¿¡æ¯ã€‚

                è¯·ç¡®ä¿ä½ çš„è¡¨æ ¼éµå¾ªæ­£ç¡®çš„Markdownè¯­æ³•ï¼š
                | åˆ—æ ‡é¢˜1 | åˆ—æ ‡é¢˜2 | åˆ—æ ‡é¢˜3 |
                | ------- | ------- | ------- |
                | æ•°æ®1   | æ•°æ®2   | æ•°æ®3   |
                """

            user_prompt = f"""
            é—®é¢˜ï¼š{question}

            ç½‘ç»œæœç´¢ç»“æœï¼š{search_background}

            æœ¬åœ°çŸ¥è¯†åº“åˆ†æï¼š{local_answer}

            {table_instruction}

            è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªç»¼åˆçš„å›ç­”ã€‚
            """

            try:
                response = client.chat.completions.create(
                    model="qwen-plus",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                )
                combined_answer = response.choices[0].message.content.strip()
                return combined_answer
            except Exception as e:
                # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œå›é€€åˆ°æœ¬åœ°ç­”æ¡ˆ
                return local_answer
        elif local_answer:
            return local_answer
        elif search_background:
            # ä»…ä»æœç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
            system_prompt = VERILOG_SYSTEM_PROMPT
            if use_table_format:
                system_prompt += "è¯·å°½å¯èƒ½ä»¥Markdownè¡¨æ ¼çš„å½¢å¼å‘ˆç°ç»“æ„åŒ–ä¿¡æ¯ã€‚"
            return generate_answer_from_deepseek(question, system_prompt=system_prompt,
                                                 background_info=f"[è”ç½‘æœç´¢ç»“æœ]ï¼š{search_background}")
        else:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

    except Exception as e:
        return f"æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}"


# ä¿®æ”¹ä»¥æ”¯æŒå¤šçŸ¥è¯†åº“çš„æµå¼å“åº”å‡½æ•°
def process_question_with_reasoning(question: str, kb_name: str = DEFAULT_KB, use_search: bool = True,
                                    use_table_format: bool = False, multi_hop: bool = False, chat_history: List = None):
    """å¢å¼ºç‰ˆprocess_questionï¼Œæ”¯æŒæµå¼å“åº”ï¼Œå®æ—¶æ˜¾ç¤ºæ£€ç´¢å’Œæ¨ç†è¿‡ç¨‹ï¼Œæ”¯æŒå¤šçŸ¥è¯†åº“å’Œå¯¹è¯å†å²"""
    try:
        kb_paths = get_kb_paths(kb_name)
        index_path = kb_paths["index_path"]
        metadata_path = kb_paths["metadata_path"]

        # æ„å»ºå¸¦å¯¹è¯å†å²çš„é—®é¢˜
        if chat_history and len(chat_history) > 0:
            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            context = "ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼š\n"
            for user_msg, assistant_msg in chat_history[-3:]:  # åªå–æœ€è¿‘3è½®å¯¹è¯
                context += f"ç”¨æˆ·ï¼š{user_msg}\n"
                context += f"åŠ©æ‰‹ï¼š{assistant_msg}\n"
            context += f"\nå½“å‰é—®é¢˜ï¼š{question}"
            enhanced_question = f"åŸºäºä»¥ä¸‹å¯¹è¯å†å²ï¼Œå›ç­”ç”¨æˆ·çš„å½“å‰é—®é¢˜ã€‚\n{context}"
        else:
            enhanced_question = question

        # åˆå§‹çŠ¶æ€
        search_result = "è”ç½‘æœç´¢è¿›è¡Œä¸­..." if use_search else "æœªå¯ç”¨è”ç½‘æœç´¢"

        if multi_hop:
            reasoning_status = f"æ­£åœ¨å‡†å¤‡å¯¹çŸ¥è¯†åº“ '{kb_name}' è¿›è¡Œå¤šè·³æ¨ç†æ£€ç´¢..."
            search_display = f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### æ¨ç†çŠ¶æ€\n{reasoning_status}"
            yield search_display, "æ­£åœ¨å¯åŠ¨å¤šè·³æ¨ç†æµç¨‹..."
        else:
            reasoning_status = f"æ­£åœ¨å‡†å¤‡å¯¹çŸ¥è¯†åº“ '{kb_name}' è¿›è¡Œå‘é‡æ£€ç´¢..."
            search_display = f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### æ£€ç´¢çŠ¶æ€\n{reasoning_status}"
            yield search_display, "æ­£åœ¨å¯åŠ¨ç®€å•æ£€ç´¢æµç¨‹..."

        # å¦‚æœå¯ç”¨ï¼Œå¹¶è¡Œè¿è¡Œæœç´¢
        search_future = None
        with ThreadPoolExecutor(max_workers=1) as executor:
            if use_search:
                search_future = executor.submit(get_search_background, question)

        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        if not (os.path.exists(index_path) and os.path.exists(metadata_path)):
            # å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œæå‰è¿”å›
            if search_future:
                # ç­‰å¾…æœç´¢ç»“æœ
                search_result = "ç­‰å¾…è”ç½‘æœç´¢ç»“æœ..."
                search_display = f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### æ£€ç´¢çŠ¶æ€\nçŸ¥è¯†åº“ '{kb_name}' ä¸­æœªæ‰¾åˆ°ç´¢å¼•"
                yield search_display, "ç­‰å¾…è”ç½‘æœç´¢ç»“æœ..."

                search_result = search_future.result() or "æœªæ‰¾åˆ°ç›¸å…³ç½‘ç»œä¿¡æ¯"
                system_prompt = VERILOG_SYSTEM_PROMPT
                if use_table_format:
                    system_prompt += "è¯·å°½å¯èƒ½ä»¥Markdownè¡¨æ ¼çš„å½¢å¼å‘ˆç°ç»“æ„åŒ–ä¿¡æ¯ã€‚"
                answer = generate_answer_from_deepseek(enhanced_question, system_prompt=system_prompt,
                                                       background_info=f"[è”ç½‘æœç´¢ç»“æœ]ï¼š{search_result}")

                search_display = f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### æ£€ç´¢çŠ¶æ€\næ— æ³•åœ¨çŸ¥è¯†åº“ '{kb_name}' ä¸­è¿›è¡Œæœ¬åœ°æ£€ç´¢ï¼ˆæœªæ‰¾åˆ°ç´¢å¼•ï¼‰"
                yield search_display, answer
            else:
                yield f"çŸ¥è¯†åº“ '{kb_name}' ä¸­æœªæ‰¾åˆ°ç´¢å¼•ï¼Œä¸”æœªå¯ç”¨è”ç½‘æœç´¢", "æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·å…ˆä¸Šä¼ æ–‡ä»¶åˆ°è¯¥çŸ¥è¯†åº“æˆ–å¯ç”¨è”ç½‘æœç´¢ã€‚"
            return

        # å¼€å§‹æµå¼å¤„ç†
        current_answer = "æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜..."

        if multi_hop:
            # ä½¿ç”¨å¤šè·³æ¨ç†çš„æµå¼æ¥å£
            reasoning_rag = ReasoningRAG(
                index_path=index_path,
                metadata_path=metadata_path,
                max_hops=3,
                initial_candidates=5,
                refined_candidates=3,
                verbose=True
            )

            # ä½¿ç”¨enhanced_questionè¿›è¡Œæ£€ç´¢
            for step_result in reasoning_rag.stream_retrieve_and_answer(enhanced_question, use_table_format):
                # æ›´æ–°å½“å‰çŠ¶æ€
                status = step_result["status"]
                reasoning_display = step_result["reasoning_display"]

                # å¦‚æœæœ‰æ–°çš„ç­”æ¡ˆï¼Œæ›´æ–°
                if step_result["answer"]:
                    current_answer = step_result["answer"]

                # å¦‚æœæœç´¢ç»“æœå·²è¿”å›ï¼Œæ›´æ–°æœç´¢ç»“æœ
                if search_future and search_future.done():
                    search_result = search_future.result() or "æœªæ‰¾åˆ°ç›¸å…³ç½‘ç»œä¿¡æ¯"

                # æ„å»ºå¹¶è¿”å›å½“å‰çŠ¶æ€
                current_display = f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æ¨ç†çŠ¶æ€\n{status}\n\n{reasoning_display}"
                yield current_display, current_answer
        else:
            # ç®€å•å‘é‡æ£€ç´¢çš„æµå¼å¤„ç†
            yield f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æ£€ç´¢çŠ¶æ€\næ­£åœ¨æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢...", "æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯..."

            # æ‰§è¡Œç®€å•å‘é‡æœç´¢ï¼Œä½¿ç”¨enhanced_question
            try:
                search_results = vector_search(enhanced_question, index_path, metadata_path, limit=5)

                if not search_results:
                    yield f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æ£€ç´¢çŠ¶æ€\næœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯", f"çŸ¥è¯†åº“ '{kb_name}' ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
                    current_answer = f"çŸ¥è¯†åº“ '{kb_name}' ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
                else:
                    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ä¿¡æ¯
                    chunks_detail = "\n\n".join(
                        [f"**ç›¸å…³ä¿¡æ¯ {i + 1}**:\n{result['chunk']}" for i, result in enumerate(search_results[:5])])
                    chunks_preview = "\n".join(
                        [f"- {result['chunk'][:100]}..." for i, result in enumerate(search_results[:3])])
                    yield f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æ£€ç´¢çŠ¶æ€\næ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³ä¿¡æ¯å—\n\n### æ£€ç´¢åˆ°çš„ä¿¡æ¯é¢„è§ˆ\n{chunks_preview}", "æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."

                    # ç”Ÿæˆç­”æ¡ˆ
                    background_chunks = "\n\n".join([f"[ç›¸å…³ä¿¡æ¯ {i + 1}]: {result['chunk']}"
                                                     for i, result in enumerate(search_results)])

                    system_prompt = VERILOG_SYSTEM_PROMPT
                    if use_table_format:
                        system_prompt += "è¯·å°½å¯èƒ½ä»¥Markdownè¡¨æ ¼çš„å½¢å¼å‘ˆç°ç»“æ„åŒ–ä¿¡æ¯ã€‚"

                    user_prompt = f"""
                    {enhanced_question}

                    èƒŒæ™¯ä¿¡æ¯ï¼š
                    {background_chunks}

                    è¯·åŸºäºä»¥ä¸ŠèƒŒæ™¯ä¿¡æ¯å’Œå¯¹è¯å†å²å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                    """

                    response = client.chat.completions.create(
                        model=Config.llm_model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ]
                    )

                    current_answer = response.choices[0].message.content.strip()
                    yield f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æ£€ç´¢çŠ¶æ€\næ£€ç´¢å®Œæˆï¼Œå·²ç”Ÿæˆç­”æ¡ˆ\n\n### æ£€ç´¢åˆ°çš„å†…å®¹\n{chunks_detail}", current_answer

            except Exception as e:
                error_msg = f"æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
                yield f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æ£€ç´¢çŠ¶æ€\n{error_msg}", f"æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
                current_answer = f"æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"

        # æ£€ç´¢å®Œæˆåï¼Œå¦‚æœæœ‰æœç´¢ç»“æœï¼Œå¯ä»¥è€ƒè™‘åˆå¹¶çŸ¥è¯†
        if search_future and search_future.done():
            search_result = search_future.result() or "æœªæ‰¾åˆ°ç›¸å…³ç½‘ç»œä¿¡æ¯"

            # å¦‚æœåŒæ—¶æœ‰æœç´¢ç»“æœå’Œæœ¬åœ°æ£€ç´¢ç»“æœï¼Œå¯ä»¥è€ƒè™‘åˆå¹¶
            if search_result and current_answer and current_answer not in ["æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...",
                                                                           "æœ¬åœ°çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"]:
                status_text = "æ­£åœ¨åˆå¹¶è”ç½‘æœç´¢å’ŒçŸ¥è¯†åº“ç»“æœ..."
                if multi_hop:
                    yield f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æ¨ç†çŠ¶æ€\n{status_text}", current_answer
                else:
                    yield f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æ£€ç´¢çŠ¶æ€\n{status_text}", current_answer

                # åˆå¹¶ç»“æœ
                system_prompt = "ä½ æ˜¯ä¸€åç¡¬ä»¶å·¥ç¨‹å¸ˆï¼Œè¯·æ•´åˆç½‘ç»œèµ„æ–™å’Œæœ¬åœ°ä»£ç åº“ï¼Œç¼–å†™ Verilog æ¨¡å—ã€‚"

                if use_table_format:
                    system_prompt += "è¯·å°½å¯èƒ½ä»¥Markdownè¡¨æ ¼çš„å½¢å¼å‘ˆç°ç»“æ„åŒ–ä¿¡æ¯ã€‚"

                user_prompt = f"""
                {enhanced_question}

                ç½‘ç»œæœç´¢ç»“æœï¼š{search_result}

                æœ¬åœ°çŸ¥è¯†åº“åˆ†æï¼š{current_answer}

                è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯å’Œå¯¹è¯å†å²ï¼Œæä¾›ä¸€ä¸ªç»¼åˆçš„å›ç­”ã€‚ç¡®ä¿ä½¿ç”¨Markdownè¡¨æ ¼æ¥å‘ˆç°é€‚åˆè¡¨æ ¼å½¢å¼çš„ä¿¡æ¯ã€‚
                """

                try:
                    response = client.chat.completions.create(
                        model="qwen-plus",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ]
                    )
                    combined_answer = response.choices[0].message.content.strip()

                    final_status = "å·²æ•´åˆè”ç½‘å’ŒçŸ¥è¯†åº“ç»“æœ"
                    if multi_hop:
                        final_display = f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æœ¬åœ°çŸ¥è¯†åº“åˆ†æ\nå·²å®Œæˆå¤šè·³æ¨ç†åˆ†æï¼Œæ£€ç´¢åˆ°çš„å†…å®¹å·²åœ¨ä¸Šæ–¹æ˜¾ç¤º\n\n### ç»¼åˆåˆ†æ\n{final_status}"
                    else:
                        # è·å–ä¹‹å‰æ£€ç´¢åˆ°çš„å†…å®¹
                        chunks_info = "".join(
                            [part.split("### æ£€ç´¢åˆ°çš„å†…å®¹\n")[-1] if "### æ£€ç´¢åˆ°çš„å†…å®¹\n" in part else "" for part in
                             search_display.split("### è”ç½‘æœç´¢ç»“æœ")])
                        if not chunks_info.strip():
                            chunks_info = "æ£€ç´¢å†…å®¹å·²åœ¨ä¸Šæ–¹æ˜¾ç¤º"
                        final_display = f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æœ¬åœ°çŸ¥è¯†åº“åˆ†æ\nå·²å®Œæˆå‘é‡æ£€ç´¢åˆ†æ\n\n### æ£€ç´¢åˆ°çš„å†…å®¹\n{chunks_info}\n\n### ç»¼åˆåˆ†æ\n{final_status}"

                    yield final_display, combined_answer
                except Exception as e:
                    # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨ç°æœ‰ç­”æ¡ˆ
                    error_status = f"åˆå¹¶ç»“æœå¤±è´¥: {str(e)}"
                    if multi_hop:
                        final_display = f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æœ¬åœ°çŸ¥è¯†åº“åˆ†æ\nå·²å®Œæˆå¤šè·³æ¨ç†åˆ†æï¼Œæ£€ç´¢åˆ°çš„å†…å®¹å·²åœ¨ä¸Šæ–¹æ˜¾ç¤º\n\n### ç»¼åˆåˆ†æ\n{error_status}"
                    else:
                        # è·å–ä¹‹å‰æ£€ç´¢åˆ°çš„å†…å®¹
                        chunks_info = "".join(
                            [part.split("### æ£€ç´¢åˆ°çš„å†…å®¹\n")[-1] if "### æ£€ç´¢åˆ°çš„å†…å®¹\n" in part else "" for part in
                             search_display.split("### è”ç½‘æœç´¢ç»“æœ")])
                        if not chunks_info.strip():
                            chunks_info = "æ£€ç´¢å†…å®¹å·²åœ¨ä¸Šæ–¹æ˜¾ç¤º"
                        final_display = f"### è”ç½‘æœç´¢ç»“æœ\n{search_result}\n\n### çŸ¥è¯†åº“: {kb_name}\n### æœ¬åœ°çŸ¥è¯†åº“åˆ†æ\nå·²å®Œæˆå‘é‡æ£€ç´¢åˆ†æ\n\n### æ£€ç´¢åˆ°çš„å†…å®¹\n{chunks_info}\n\n### ç»¼åˆåˆ†æ\n{error_status}"

                    yield final_display, current_answer

    except Exception as e:
        error_msg = f"å¤„ç†å¤±è´¥ï¼š{str(e)}\n{traceback.format_exc()}"
        yield f"### é”™è¯¯ä¿¡æ¯\n{error_msg}", f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"


# æ·»åŠ å¤„ç†å‡½æ•°ï¼Œæ‰¹é‡ä¸Šä¼ æ–‡ä»¶åˆ°æŒ‡å®šçŸ¥è¯†åº“
def batch_upload_to_kb(file_objs: List, kb_name: str) -> str:
    """æ‰¹é‡ä¸Šä¼ æ–‡ä»¶åˆ°æŒ‡å®šçŸ¥è¯†åº“å¹¶è¿›è¡Œå¤„ç†"""
    print(f"ğŸ‘‰ [Debug] æ”¶åˆ°ä¸Šä¼ è¯·æ±‚ï¼KBåç§°: {kb_name}, æ–‡ä»¶æ•°é‡: {len(file_objs) if file_objs else 0}", flush=True)
    try:
        if not kb_name or not kb_name.strip():
            return "é”™è¯¯ï¼šæœªæŒ‡å®šçŸ¥è¯†åº“"

        # ç¡®ä¿çŸ¥è¯†åº“ç›®å½•å­˜åœ¨
        kb_dir = os.path.join(KB_BASE_DIR, kb_name)
        if not os.path.exists(kb_dir):
            os.makedirs(kb_dir, exist_ok=True)

        if not file_objs or len(file_objs) == 0:
            return "é”™è¯¯ï¼šæœªé€‰æ‹©ä»»ä½•æ–‡ä»¶"

        return process_and_index_files(file_objs, kb_name)
    except Exception as e:
        return f"ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“å¤±è´¥: {str(e)}"


# Gradio ç•Œé¢ - ä¿®æ”¹ä¸ºæ”¯æŒå¤šçŸ¥è¯†åº“
custom_css = """
.web-search-toggle .form { display: flex !important; align-items: center !important; }
.web-search-toggle .form > label { order: 2 !important; margin-left: 10px !important; }
.web-search-toggle .checkbox-wrap { order: 1 !important; background: #d4e4d4 !important; border-radius: 15px !important; padding: 2px !important; width: 50px !important; height: 28px !important; }
.web-search-toggle .checkbox-wrap .checkbox-container { width: 24px !important; height: 24px !important; transition: all 0.3s ease !important; }
.web-search-toggle input:checked + .checkbox-wrap { background: #2196F3 !important; }
.web-search-toggle input:checked + .checkbox-wrap .checkbox-container { transform: translateX(22px) !important; }
#search-results { max-height: 400px; overflow-y: auto; border: 1px solid #2196F3; border-radius: 5px; padding: 10px; background-color: #e7f0f9; }
#question-input { border-color: #2196F3 !important; }
#answer-output { background-color: #f0f7f0; border-color: #2196F3 !important; max-height: 400px; overflow-y: auto; }
.submit-btn { background-color: #2196F3 !important; border: none !important; }
.reasoning-steps { background-color: #f0f7f0; border: 1px dashed #2196F3; padding: 10px; margin-top: 10px; border-radius: 5px; }
.loading-spinner { display: inline-block; width: 20px; height: 20px; border: 3px solid rgba(33, 150, 243, 0.3); border-radius: 50%; border-top-color: #2196F3; animation: spin 1s ease-in-out infinite; }
@keyframes spin { to { transform: rotate(360deg); } }
.stream-update { animation: fade 0.5s ease-in-out; }
@keyframes fade { from { background-color: rgba(33, 150, 243, 0.1); } to { background-color: transparent; } }
.status-box { padding: 10px; border-radius: 5px; margin-bottom: 10px; font-weight: bold; }
.status-processing { background-color: #e3f2fd; color: #1565c0; border-left: 4px solid #2196F3; }
.status-success { background-color: #e8f5e9; color: #2e7d32; border-left: 4px solid #4CAF50; }
.status-error { background-color: #ffebee; color: #c62828; border-left: 4px solid #f44336; }
.multi-hop-toggle .form { display: flex !important; align-items: center !important; }
.multi-hop-toggle .form > label { order: 2 !important; margin-left: 10px !important; }
.multi-hop-toggle .checkbox-wrap { order: 1 !important; background: #d4e4d4 !important; border-radius: 15px !important; padding: 2px !important; width: 50px !important; height: 28px !important; }
.multi-hop-toggle .checkbox-wrap .checkbox-container { width: 24px !important; height: 24px !important; transition: all 0.3s ease !important; }
.multi-hop-toggle input:checked + .checkbox-wrap { background: #4CAF50 !important; }
.multi-hop-toggle input:checked + .checkbox-wrap .checkbox-container { transform: translateX(22px) !important; }
.kb-management { border: 1px solid #2196F3; border-radius: 5px; padding: 15px; margin-bottom: 15px; background-color: #f0f7ff; }
.kb-selector { margin-bottom: 10px; }
/* ç¼©å°æ–‡ä»¶ä¸Šä¼ åŒºåŸŸé«˜åº¦ */
.compact-upload {
    margin-bottom: 10px;
}

.file-upload.compact {
    padding: 10px;  /* å‡å°å†…è¾¹è· */
    min-height: 120px; /* å‡å°æœ€å°é«˜åº¦ */
    margin-bottom: 10px;
}

/* ä¼˜åŒ–çŸ¥è¯†åº“å†…å®¹æ˜¾ç¤ºåŒºåŸŸ */
.kb-files-list {
    height: 400px;
    overflow-y: auto;
}

/* ç¡®ä¿å³ä¾§åˆ—æœ‰è¶³å¤Ÿç©ºé—´ */
#kb-files-group {
    height: 100%;
    display: flex;
    flex-direction: column;
}
.kb-files-list { max-height: 250px; overflow-y: auto; border: 1px solid #ccc; border-radius: 5px; padding: 10px; margin-top: 10px; background-color: #f9f9f9; }
#kb-management-container {
    max-width: 800px !important;
    margin: 0 !important; /* ç§»é™¤è‡ªåŠ¨è¾¹è·ï¼Œé å·¦å¯¹é½ */
    margin-left: 20px !important; /* æ·»åŠ å·¦è¾¹è· */
}
.container {
    max-width: 1200px !important;
    margin: 0 auto !important;
}
.file-upload {
    border: 2px dashed #2196F3;
    padding: 15px;
    border-radius: 10px;
    background-color: #f0f7ff;
    margin-bottom: 15px;
}
.tabs.tab-selected {
    background-color: #e3f2fd;
    border-bottom: 3px solid #2196F3;
}
.group {
    border: 1px solid #e0e0e0;
    border-radius: 8px;
    padding: 10px;
    margin-bottom: 15px;
    background-color: #fafafa;
}

/* æ·»åŠ æ›´å¤šé’ˆå¯¹çŸ¥è¯†åº“ç®¡ç†é¡µé¢çš„æ ·å¼ */
#kb-controls, #kb-file-upload, #kb-files-group {
    width: 100% !important;
    max-width: 800px !important;
    margin-right: auto !important;
}

/* ä¿®æ”¹Gradioé»˜è®¤çš„æ ‡ç­¾é¡µæ ·å¼ä»¥æ”¯æŒå·¦å¯¹é½ */
.tabs > .tab-nav > button {
    flex: 0 1 auto !important; /* ä¿®æ”¹ä¸ºä¸è‡ªåŠ¨æ‰©å±•ï¼Œåªå ç”¨å¿…è¦ç©ºé—´ */
}
.tabs > .tabitem {
    padding-left: 0 !important; /* ç§»é™¤å·¦è¾¹è·ï¼Œä½¿å†…å®¹é å·¦ */
}
/* å¯¹äºé¦–é¡µçš„é¡¶éƒ¨æ ‡é¢˜éƒ¨åˆ† */
#app-container h1, #app-container h2, #app-container h3, 
#app-container > .prose {
    text-align: left !important;
    padding-left: 20px !important;
}
"""

custom_theme = gr.themes.Soft(
    primary_hue="blue",
    secondary_hue="blue",
    neutral_hue="gray",
    text_size="lg",
    spacing_size="md",
    radius_size="md"
)

# æ·»åŠ ç®€å•çš„JavaScriptï¼Œé€šè¿‡htmlç»„ä»¶å®ç°
js_code = """
<script>
document.addEventListener('DOMContentLoaded', function() {
    // å½“é¡µé¢åŠ è½½å®Œæ¯•åï¼Œæ‰¾åˆ°æäº¤æŒ‰é’®ï¼Œå¹¶ä¸ºå…¶æ·»åŠ ç‚¹å‡»äº‹ä»¶
    const observer = new MutationObserver(function(mutations) {
        // æ‰¾åˆ°æäº¤æŒ‰é’®
        const submitButton = document.querySelector('button[data-testid="submit"]');
        if (submitButton) {
            submitButton.addEventListener('click', function() {
                // æ‰¾åˆ°æ£€ç´¢æ ‡ç­¾é¡µæŒ‰é’®å¹¶ç‚¹å‡»å®ƒ
                setTimeout(function() {
                    const retrievalTab = document.querySelector('[data-testid="tab-button-retrieval-tab"]');
                    if (retrievalTab) retrievalTab.click();
                }, 100);
            });
            observer.disconnect(); // ä¸€æ—¦æ‰¾åˆ°å¹¶è®¾ç½®äº‹ä»¶ï¼Œåœæ­¢è§‚å¯Ÿ
        }
    });

    // å¼€å§‹è§‚å¯Ÿæ–‡æ¡£å˜åŒ–
    observer.observe(document.body, { childList: true, subtree: true });
});
</script>
"""

with gr.Blocks(title="Verilog ä»£ç ç”ŸæˆåŠ©æ‰‹", theme=custom_theme, css=custom_css, elem_id="app-container") as demo:
    with gr.Column(elem_id="header-container"):
        gr.Markdown("""
        # âš¡ Verilog ä»£ç ç”ŸæˆåŠ©æ‰‹
        **åŸºäº RAG çš„ç¡¬ä»¶å¼€å‘ Copilot** ä¸Šä¼ ä½ çš„ Verilog/SystemVerilog ä»£ç åº“ï¼Œé€šè¿‡è¯­ä¹‰æ£€ç´¢å’Œæ¨ç†ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¯ç»¼åˆä»£ç ã€‚
        """)

    # æ·»åŠ JavaScriptè„šæœ¬
    gr.HTML(js_code, visible=False)

    # ä½¿ç”¨Stateæ¥å­˜å‚¨å¯¹è¯å†å²
    chat_history_state = gr.State([])

    # åˆ›å»ºæ ‡ç­¾é¡µ
    with gr.Tabs() as tabs:
        # çŸ¥è¯†åº“ç®¡ç†æ ‡ç­¾é¡µ
        with gr.TabItem("çŸ¥è¯†åº“ç®¡ç†"):
            with gr.Row():
                # å·¦ä¾§åˆ—ï¼šæ§åˆ¶åŒº
                with gr.Column(scale=1, min_width=400):
                    gr.Markdown("### ğŸ“š çŸ¥è¯†åº“ç®¡ç†ä¸æ„å»º")

                    with gr.Row(elem_id="kb-controls"):
                        with gr.Column(scale=1):
                            new_kb_name = gr.Textbox(
                                label="æ–°çŸ¥è¯†åº“åç§°",
                                placeholder="è¾“å…¥æ–°çŸ¥è¯†åº“åç§°",
                                lines=1
                            )
                            create_kb_btn = gr.Button("åˆ›å»ºçŸ¥è¯†åº“", variant="primary", scale=1)

                        with gr.Column(scale=1):
                            current_kbs = get_knowledge_bases()
                            kb_dropdown = gr.Dropdown(
                                label="é€‰æ‹©çŸ¥è¯†åº“",
                                choices=current_kbs,
                                value=DEFAULT_KB if DEFAULT_KB in current_kbs else (
                                    current_kbs[0] if current_kbs else None),
                                elem_classes="kb-selector"
                            )

                            with gr.Row():
                                refresh_kb_btn = gr.Button("åˆ·æ–°åˆ—è¡¨", size="sm", scale=1)
                                delete_kb_btn = gr.Button("åˆ é™¤çŸ¥è¯†åº“", size="sm", variant="stop", scale=1)

                    kb_status = gr.Textbox(label="çŸ¥è¯†åº“çŠ¶æ€", interactive=False, placeholder="é€‰æ‹©æˆ–åˆ›å»ºçŸ¥è¯†åº“")

                    with gr.Group(elem_id="kb-file-upload", elem_classes="compact-upload"):
                        gr.Markdown("### ğŸ“„ ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“")
                        file_upload = gr.File(
                            label="é€‰æ‹©æ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰TXT/PDFï¼‰",
                            type="file",
                            file_types=[".txt", ".pdf",".v",".sv"],
                            file_count="multiple",
                            elem_classes="file-upload compact"
                        )
                        upload_status = gr.Textbox(label="ä¸Šä¼ çŠ¶æ€", interactive=False, placeholder="ä¸Šä¼ åæ˜¾ç¤ºçŠ¶æ€")

                    kb_select_for_chat = gr.Dropdown(
                        label="ä¸ºå¯¹è¯é€‰æ‹©çŸ¥è¯†åº“",
                        choices=current_kbs,
                        value=DEFAULT_KB if DEFAULT_KB in current_kbs else (current_kbs[0] if current_kbs else None),
                        visible=False  # éšè—ï¼Œä»…ç”¨äºåŒæ­¥
                    )

                with gr.Column(scale=1, min_width=400):
                    with gr.Group(elem_id="kb-files-group"):
                        gr.Markdown("### ğŸ“‹ çŸ¥è¯†åº“å†…å®¹")
                        kb_files_list = gr.Markdown(
                            value="é€‰æ‹©çŸ¥è¯†åº“æŸ¥çœ‹æ–‡ä»¶...",
                            elem_classes="kb-files-list"
                        )

                # ç”¨äºå¯¹è¯ç•Œé¢çš„çŸ¥è¯†åº“é€‰æ‹©å™¨
                kb_select_for_chat = gr.Dropdown(
                    label="ä¸ºå¯¹è¯é€‰æ‹©çŸ¥è¯†åº“",
                    choices=current_kbs,
                    value=DEFAULT_KB if DEFAULT_KB in current_kbs else (current_kbs[0] if current_kbs else None),
                    visible=False  # éšè—ï¼Œä»…ç”¨äºåŒæ­¥
                )

        # å¯¹è¯äº¤äº’æ ‡ç­¾é¡µ
        with gr.TabItem("å¯¹è¯äº¤äº’"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ å¯¹è¯è®¾ç½®")

                    kb_dropdown_chat = gr.Dropdown(
                        label="é€‰æ‹©çŸ¥è¯†åº“è¿›è¡Œå¯¹è¯",
                        choices=current_kbs,
                        value=DEFAULT_KB if DEFAULT_KB in current_kbs else (current_kbs[0] if current_kbs else None),
                    )

                    with gr.Row():
                        web_search_toggle = gr.Checkbox(
                            label="ğŸŒ å¯ç”¨è”ç½‘æœç´¢",
                            value=True,
                            info="è·å–æœ€æ–°åŒ»ç–—åŠ¨æ€",
                            elem_classes="web-search-toggle"
                        )
                        table_format_toggle = gr.Checkbox(
                            label="ğŸ“Š è¡¨æ ¼æ ¼å¼è¾“å‡º",
                            value=True,
                            info="ä½¿ç”¨Markdownè¡¨æ ¼å±•ç¤ºç»“æ„åŒ–å›ç­”",
                            elem_classes="web-search-toggle"
                        )

                    multi_hop_toggle = gr.Checkbox(
                        label="ğŸ”„ å¯ç”¨å¤šè·³æ¨ç†",
                        value=False,
                        info="ä½¿ç”¨é«˜çº§å¤šè·³æ¨ç†æœºåˆ¶ï¼ˆè¾ƒæ…¢ä½†æ›´å…¨é¢ï¼‰",
                        elem_classes="multi-hop-toggle"
                    )

                    with gr.Accordion("æ˜¾ç¤ºæ£€ç´¢è¿›å±•", open=False):
                        search_results_output = gr.Markdown(
                            label="æ£€ç´¢è¿‡ç¨‹",
                            elem_id="search-results",
                            value="ç­‰å¾…æäº¤é—®é¢˜..."
                        )

                with gr.Column(scale=3):
                    gr.Markdown("### ğŸ’¬ å¯¹è¯å†å²")
                    chatbot = gr.Chatbot(
                        elem_id="chatbot",
                        label="å¯¹è¯å†å²",
                        height=550
                    )

            with gr.Row():
                question_input = gr.Textbox(
                    label="è¾“å…¥éœ€è¦ç”Ÿæˆverilogçš„è¦æ±‚",
                    placeholder="ä¾‹å¦‚ï¼šå†™ä¸€ä¸ªå¸¦æœ‰å¼‚æ­¥å¤ä½çš„ 8 ä½è®¡æ•°å™¨",
                    lines=2,
                    elem_id="question-input"
                )

            with gr.Row(elem_classes="submit-row"):
                submit_btn = gr.Button("æäº¤é—®é¢˜", variant="primary", elem_classes="submit-btn")
                clear_btn = gr.Button("æ¸…ç©ºè¾“å…¥", variant="secondary")
                clear_history_btn = gr.Button("æ¸…ç©ºå¯¹è¯å†å²", variant="secondary", elem_classes="clear-history-btn")

            # çŠ¶æ€æ˜¾ç¤ºæ¡†
            status_box = gr.HTML(
                value='<div class="status-box status-processing">å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…æ‚¨çš„é—®é¢˜</div>',
                visible=True
            )

            gr.Examples(
                examples=[
                    ["å†™ä¸€ä¸ªå¸¦æœ‰å¼‚æ­¥å¤ä½çš„ 8 ä½è®¡æ•°å™¨"],
                    ["ç”Ÿæˆä¸€ä¸ª AXI4-Lite Slave æ¥å£æ¨¡æ¿"],
                    ["å¦‚ä½•å®ç°è·¨æ—¶é’ŸåŸŸçš„å•æ¯”ç‰¹ä¿¡å·åŒæ­¥ï¼Ÿ"],
                    ["å†™ä¸€ä¸ªç”¨äºè§†é¢‘å¤„ç†çš„ Line Buffer"],
                    ["è§£é‡Šä¸€ä¸‹è¿™æ®µä»£ç ä¸­çš„çŠ¶æ€æœºé€»è¾‘"]
                ],
                inputs=question_input,
                label="ç¤ºä¾‹é—®é¢˜ï¼ˆç‚¹å‡»å°è¯•ï¼‰"
            )


    # åˆ›å»ºçŸ¥è¯†åº“å‡½æ•°
    def create_kb_and_refresh(kb_name):
        result = create_knowledge_base(kb_name)
        kbs = get_knowledge_bases()
        # æ›´æ–°ä¸¤ä¸ªä¸‹æ‹‰èœå•
        return result, gr.update(choices=kbs, value=kb_name if "åˆ›å»ºæˆåŠŸ" in result else None), gr.update(choices=kbs,
                                                                                                          value=kb_name if "åˆ›å»ºæˆåŠŸ" in result else None)


    # åˆ·æ–°çŸ¥è¯†åº“åˆ—è¡¨
    def refresh_kb_list():
        kbs = get_knowledge_bases()
        # æ›´æ–°ä¸¤ä¸ªä¸‹æ‹‰èœå•
        return gr.update(choices=kbs, value=kbs[0] if kbs else None), gr.update(choices=kbs,
                                                                                value=kbs[0] if kbs else None)


    # åˆ é™¤çŸ¥è¯†åº“
    def delete_kb_and_refresh(kb_name):
        result = delete_knowledge_base(kb_name)
        kbs = get_knowledge_bases()
        # æ›´æ–°ä¸¤ä¸ªä¸‹æ‹‰èœå•
        return result, gr.update(choices=kbs, value=kbs[0] if kbs else None), gr.update(choices=kbs,
                                                                                        value=kbs[0] if kbs else None)


    # æ›´æ–°çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨
    def update_kb_files_list(kb_name):
        if not kb_name:
            return "æœªé€‰æ‹©çŸ¥è¯†åº“"

        files = get_kb_files(kb_name)
        kb_dir = os.path.join(KB_BASE_DIR, kb_name)
        has_index = os.path.exists(os.path.join(kb_dir, "semantic_chunk.index"))

        if not files:
            files_str = "çŸ¥è¯†åº“ä¸­æš‚æ— æ–‡ä»¶"
        else:
            files_str = "**æ–‡ä»¶åˆ—è¡¨:**\n\n" + "\n".join([f"- {file}" for file in files])

        index_status = "\n\n**ç´¢å¼•çŠ¶æ€:** " + ("âœ… å·²å»ºç«‹ç´¢å¼•" if has_index else "âŒ æœªå»ºç«‹ç´¢å¼•")

        return f"### çŸ¥è¯†åº“: {kb_name}\n\n{files_str}{index_status}"


    # åŒæ­¥çŸ¥è¯†åº“é€‰æ‹© - ç®¡ç†ç•Œé¢åˆ°å¯¹è¯ç•Œé¢
    def sync_kb_to_chat(kb_name):
        return gr.update(value=kb_name)


    # åŒæ­¥çŸ¥è¯†åº“é€‰æ‹© - å¯¹è¯ç•Œé¢åˆ°ç®¡ç†ç•Œé¢
    def sync_chat_to_kb(kb_name):
        return gr.update(value=kb_name), update_kb_files_list(kb_name)


    # å¤„ç†æ–‡ä»¶ä¸Šä¼ åˆ°æŒ‡å®šçŸ¥è¯†åº“
    def process_upload_to_kb(files, kb_name):
        if not kb_name:
            return "é”™è¯¯ï¼šæœªé€‰æ‹©çŸ¥è¯†åº“",None

        result = batch_upload_to_kb(files, kb_name)
        # æ›´æ–°çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨
        files_list = update_kb_files_list(kb_name)
        return result, files_list


    # çŸ¥è¯†åº“é€‰æ‹©å˜åŒ–æ—¶
    def on_kb_change(kb_name):
        if not kb_name:
            return "æœªé€‰æ‹©çŸ¥è¯†åº“", "é€‰æ‹©çŸ¥è¯†åº“æŸ¥çœ‹æ–‡ä»¶..."

        kb_dir = os.path.join(KB_BASE_DIR, kb_name)
        has_index = os.path.exists(os.path.join(kb_dir, "semantic_chunk.index"))
        status = f"å·²é€‰æ‹©çŸ¥è¯†åº“: {kb_name}" + (" (å·²å»ºç«‹ç´¢å¼•)" if has_index else " (æœªå»ºç«‹ç´¢å¼•)")

        # æ›´æ–°æ–‡ä»¶åˆ—è¡¨
        files_list = update_kb_files_list(kb_name)

        return status, files_list


    # åˆ›å»ºçŸ¥è¯†åº“æŒ‰é’®åŠŸèƒ½
    create_kb_btn.click(
        fn=create_kb_and_refresh,
        inputs=[new_kb_name],
        outputs=[kb_status, kb_dropdown, kb_dropdown_chat]
    ).then(
        fn=lambda: "",  # æ¸…ç©ºè¾“å…¥æ¡†
        inputs=[],
        outputs=[new_kb_name]
    )

    # åˆ·æ–°çŸ¥è¯†åº“åˆ—è¡¨æŒ‰é’®åŠŸèƒ½
    refresh_kb_btn.click(
        fn=refresh_kb_list,
        inputs=[],
        outputs=[kb_dropdown, kb_dropdown_chat]
    )

    # åˆ é™¤çŸ¥è¯†åº“æŒ‰é’®åŠŸèƒ½
    delete_kb_btn.click(
        fn=delete_kb_and_refresh,
        inputs=[kb_dropdown],
        outputs=[kb_status, kb_dropdown, kb_dropdown_chat]
    ).then(
        fn=update_kb_files_list,
        inputs=[kb_dropdown],
        outputs=[kb_files_list]
    )

    # çŸ¥è¯†åº“é€‰æ‹©å˜åŒ–æ—¶ - ç®¡ç†ç•Œé¢
    kb_dropdown.change(
        fn=on_kb_change,
        inputs=[kb_dropdown],
        outputs=[kb_status, kb_files_list]
    ).then(
        fn=sync_kb_to_chat,
        inputs=[kb_dropdown],
        outputs=[kb_dropdown_chat]
    )

    # çŸ¥è¯†åº“é€‰æ‹©å˜åŒ–æ—¶ - å¯¹è¯ç•Œé¢
    kb_dropdown_chat.change(
        fn=sync_chat_to_kb,
        inputs=[kb_dropdown_chat],
        outputs=[kb_dropdown, kb_files_list]
    )

    # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
    file_upload.upload(
        fn=process_upload_to_kb,
        inputs=[file_upload, kb_dropdown],
        outputs=[upload_status, kb_files_list]
    )

    # æ¸…ç©ºè¾“å…¥æŒ‰é’®åŠŸèƒ½
    clear_btn.click(
        fn=lambda: "",
        inputs=[],
        outputs=[question_input]
    )


    # æ¸…ç©ºå¯¹è¯å†å²æŒ‰é’®åŠŸèƒ½
    def clear_history():
        return [], []


    clear_history_btn.click(
        fn=clear_history,
        inputs=[],
        outputs=[chatbot, chat_history_state]
    )


    # æäº¤æŒ‰é’® - å¼€å§‹æµå¼å¤„ç†
    def update_status(is_processing=True, is_error=False):
        if is_processing:
            return '<div class="status-box status-processing">æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜...</div>'
        elif is_error:
            return '<div class="status-box status-error">å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯</div>'
        else:
            return '<div class="status-box status-success">å›ç­”å·²ç”Ÿæˆå®Œæ¯•</div>'


    # å¤„ç†é—®é¢˜å¹¶æ›´æ–°å¯¹è¯å†å²
    def process_and_update_chat(question, kb_name, use_search, use_table_format, multi_hop, chat_history):
        if not question.strip():
            return chat_history, update_status(False, True), "ç­‰å¾…æäº¤é—®é¢˜..."

        try:
            # é¦–å…ˆæ›´æ–°èŠå¤©ç•Œé¢ï¼Œæ˜¾ç¤ºç”¨æˆ·é—®é¢˜
            chat_history.append([question, "æ­£åœ¨æ€è€ƒ..."])
            yield chat_history, update_status(True), f"å¼€å§‹å¤„ç†æ‚¨çš„é—®é¢˜ï¼Œä½¿ç”¨çŸ¥è¯†åº“: {kb_name}..."

            # ç”¨äºç´¯ç§¯æ£€ç´¢çŠ¶æ€å’Œç­”æ¡ˆ
            last_search_display = ""
            last_answer = ""

            # ä½¿ç”¨ç”Ÿæˆå™¨è¿›è¡Œæµå¼å¤„ç†
            for search_display, answer in process_question_with_reasoning(question, kb_name, use_search,
                                                                          use_table_format, multi_hop,
                                                                          chat_history[:-1]):
                # æ›´æ–°æ£€ç´¢çŠ¶æ€å’Œç­”æ¡ˆ
                last_search_display = search_display
                last_answer = answer

                # æ›´æ–°èŠå¤©å†å²ä¸­çš„æœ€åä¸€æ¡ï¼ˆå½“å‰çš„å›ç­”ï¼‰
                if chat_history:
                    chat_history[-1][1] = answer
                    yield chat_history, update_status(True), search_display

            # å¤„ç†å®Œæˆï¼Œæ›´æ–°çŠ¶æ€
            yield chat_history, update_status(False), last_search_display

        except Exception as e:
            # å‘ç”Ÿé”™è¯¯æ—¶æ›´æ–°çŠ¶æ€å’ŒèŠå¤©å†å²
            error_msg = f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}"
            if chat_history:
                chat_history[-1][1] = error_msg
            yield chat_history, update_status(False, True), f"### é”™è¯¯\n{error_msg}"


    # è¿æ¥æäº¤æŒ‰é’®
    submit_btn.click(
        fn=process_and_update_chat,
        inputs=[question_input, kb_dropdown_chat, web_search_toggle, table_format_toggle, multi_hop_toggle,
                chat_history_state],
        outputs=[chatbot, status_box, search_results_output],
        queue=True
    ).then(
        fn=lambda: "",  # æ¸…ç©ºè¾“å…¥æ¡†
        inputs=[],
        outputs=[question_input]
    ).then(
        fn=lambda h: h,  # æ›´æ–°state
        inputs=[chatbot],
        outputs=[chat_history_state]
    )

    # æ”¯æŒEnteré”®æäº¤
    question_input.submit(
        fn=process_and_update_chat,
        inputs=[question_input, kb_dropdown_chat, web_search_toggle, table_format_toggle, multi_hop_toggle,
                chat_history_state],
        outputs=[chatbot, status_box, search_results_output],
        queue=True
    ).then(
        fn=lambda: "",  # æ¸…ç©ºè¾“å…¥æ¡†
        inputs=[],
        outputs=[question_input]
    ).then(
        fn=lambda h: h,  # æ›´æ–°state
        inputs=[chatbot],
        outputs=[chat_history_state]
    )

if __name__ == "__main__":
    demo.queue().launch(server_name="0.0.0.0", server_port=7860, share=False, inbrowser=True)