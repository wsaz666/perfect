import os
import torch
from functional import seq
import numpy as np
import torch.nn.functional as F
from torch import cosine_similarity
from config import Config
from openai import OpenAI
from transformers import AutoTokenizer, AutoModel

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


class TextVector():
    def __init__(self, cfg):
        self.bert_path = cfg.bert_path

        # ä»é…ç½®æ–‡ä»¶è¯»å–APIç›¸å…³è®¾ç½®
        self.use_api = getattr(cfg, 'use_api', True)
        self.api_key = getattr(cfg, 'api_key', "sk-5b45aa67249a44d38abca3c02cc78a70")
        self.base_url = getattr(cfg, 'base_url', "https://dashscope.aliyuncs.com/compatible-mode/v1")
        self.model_name = getattr(cfg, 'model_name', "text-embedding-v3")
        self.dimensions = getattr(cfg, 'dimensions', 1024)
        self.batch_size = getattr(cfg, 'batch_size', 10)

        # åªæœ‰åœ¨ä¸ä½¿ç”¨APIæ—¶æ‰åŠ è½½æœ¬åœ°æ¨¡å‹
        if not self.use_api:
            self.load_model()

    def load_model(self):
        """è½½å…¥æ¨¡å‹ï¼ˆå·²æ·»åŠ  GPU æ”¯æŒï¼‰"""
        print(f"ğŸ‘‰ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.bert_path}", flush=True)
        self.tokenizer = AutoTokenizer.from_pretrained(self.bert_path)

        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ’» è¿è¡Œè®¾å¤‡: {self.device} (å¦‚æœæ˜¯ cpu ä¼šå¾ˆæ…¢ï¼Œå»ºè®®ä½¿ç”¨ NVIDIA æ˜¾å¡)", flush=True)

        self.model = AutoModel.from_pretrained(self.bert_path).to(self.device)

    def mean_pooling(self, model_output, attention_mask):
        """é‡‡ç”¨åºåˆ—mean-poolingè·å¾—å¥å­çš„è¡¨å¾å‘é‡"""
        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def get_vec(self, sentences):
        """é€šè¿‡æ¨¡å‹è·å–å¥å­çš„å‘é‡"""
        if self.use_api:
            return self.get_vec_api(sentences)

        # Tokenize
        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

        # âœ… ã€å…³é”®ã€‘æŠŠè¾“å…¥æ•°æ®æ¬åˆ° GPU ä¸Š
        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}

        with torch.no_grad():
            model_output = self.model(**encoded_input)

        sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])

        # æ¬å› CPU è½¬æˆ list
        sentence_embeddings = sentence_embeddings.cpu().numpy().tolist()
        return sentence_embeddings

    def get_vec_api(self, query, batch_size=None):
        """é€šè¿‡APIè·å–å¥å­çš„å‘é‡"""
        if batch_size is None:
            batch_size = self.batch_size

        # ç©ºæŸ¥è¯¢æ£€æŸ¥
        if not query:
            print("Warning: Empty query provided to get_vec_api")
            return []

        client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        if isinstance(query, str):
            query = [query]

        # ç§»é™¤ç©ºå­—ç¬¦ä¸²å’ŒNoneå€¼ï¼Œç¡®ä¿è¾“å…¥æ•°æ®æœ‰æ•ˆ
        query = [q for q in query if q and isinstance(q, str) and q.strip()]
        if not query:
            print("Warning: No valid text to vectorize after filtering")
            return []

        all_vectors = []
        retry_count = 0
        max_retries = 2  # å…è®¸é‡è¯•å‡ æ¬¡

        while retry_count <= max_retries and not all_vectors:
            try:
                for i in range(0, len(query), batch_size):
                    batch = query[i:i + batch_size]
                    try:
                        completion = client.embeddings.create(
                            model=self.model_name,
                            input=batch,
                            dimensions=self.dimensions,
                            encoding_format="float"
                        )
                        vectors = [embedding.embedding for embedding in completion.data]
                        all_vectors.extend(vectors)
                    except Exception as e:
                        print(f"å‘é‡åŒ–æ‰¹æ¬¡ {i // batch_size + 1} å¤±è´¥ï¼š{str(e)}")
                        # ä¸ç«‹å³è¿”å›ç©ºæ•°ç»„ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ‰¹æ¬¡
                        continue

                # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸè·å–çš„å‘é‡
                if all_vectors:
                    break
                else:
                    retry_count += 1
                    print(f"æœªè·å–åˆ°ä»»ä½•å‘é‡ï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•...")

            except Exception as outer_e:
                print(f"å‘é‡åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(outer_e)}")
                retry_count += 1
                if retry_count <= max_retries:
                    print(f"ç¬¬ {retry_count} æ¬¡é‡è¯•...")

        # è¿”å›å‘é‡æ•°ç»„ï¼Œå¦‚æœä»ç„¶ä¸ºç©ºï¼Œç¡®ä¿è¿”å›ä¸€ä¸ªæ­£ç¡®å½¢çŠ¶çš„ç©ºæ•°ç»„
        if not all_vectors and self.dimensions > 0:
            print("Warning: è¿”å›ä¸€ä¸ªç©ºçš„å‘é‡æ•°ç»„ï¼Œå½¢çŠ¶ä¸º [0, dimensions]")
            return np.zeros((0, self.dimensions))

        return all_vectors

        # --- è¯·å¤åˆ¶å¹¶è¦†ç›– text2vec.py é‡Œçš„ get_vec_batch å‡½æ•° ---
    def get_vec_batch(self, data, bs=None):
        """batchæ–¹å¼è·å–ï¼Œæé«˜æ•ˆç‡ (å·²æ·»åŠ è¿›åº¦æ¡æ˜¾ç¤º)"""
        if bs is None:
            bs = self.batch_size

        if self.use_api:
            # å¦‚æœä½¿ç”¨APIï¼Œç›´æ¥è°ƒç”¨APIæ–¹æ³•
            vectors = self.get_vec_api(data, bs)
            return torch.tensor(np.array(vectors)) if len(vectors) > 0 else torch.tensor(np.array([]))

        # å¦åˆ™ä½¿ç”¨åŸå§‹BERTæ–¹æ³• (æœ¬åœ°æ¨¡å‹)
        # ğŸ‘‡ ä¿®æ”¹ç‚¹ï¼šä¸å†ç”¨ seq åº“ï¼Œæ”¹ç”¨åŸç”Ÿå¾ªç¯ä»¥ä¾¿æ‰“å°è¿›åº¦
        all_vectors = []
        total_len = len(data)
        total_batches = (total_len - 1) // bs + 1

        print(f"ğŸ“Š [æœ¬åœ°æ¨¡å‹] å¼€å§‹å¤„ç† {total_len} æ¡æ•°æ®ï¼Œå…± {total_batches} ä¸ªæ‰¹æ¬¡...", flush=True)

        for i in range(0, total_len, bs):
            batch = data[i:i + bs]
            current_batch = i // bs + 1

            # æ¯å¤„ç† 1 ä¸ªæ‰¹æ¬¡å°±æ‰“å°ä¸€æ¬¡ï¼ˆåŠ ä¸Š flush=True ç¡®ä¿å®æ—¶æ˜¾ç¤ºï¼‰
            # å¦‚æœåˆ·å±å¤ªå¿«ï¼Œå¯ä»¥æ”¹æˆ if current_batch % 10 == 0:
            print(f"   âš¡ è¿›åº¦: {current_batch}/{total_batches} | å·²å®Œæˆ: {current_batch / total_batches:.2%}",
                    flush=True)

            vecs = self.get_vec(batch)
            all_vectors.extend(vecs)

        all_vectors = torch.tensor(np.array(all_vectors))
        return all_vectors

    def vector_similarity(self, vectors):
        """ä»¥[queryï¼Œtext1ï¼Œtext2...]æ¥è®¡ç®—queryä¸text1ï¼Œtext2,...çš„cosineç›¸ä¼¼åº¦"""
        # Add dimension checking to prevent errors
        if vectors.size(0) <= 1:
            print("Warning: Not enough vectors for similarity calculation")
            return []

        if len(vectors.shape) < 2:
            print("Warning: Vectors must be 2-dimensional")
            return []

        vectors = F.normalize(vectors, p=2, dim=1)
        q_vec = vectors[0, :]
        o_vec = vectors[1:, :]
        sim = cosine_similarity(q_vec, o_vec)
        sim = sim.data.cpu().numpy().tolist()
        return sim


# ä¿®æ­£å‡½æ•°åæ‹¼å†™é”™è¯¯ï¼šget_vec_bath -> get_vec_batch
cfg = Config()
tv = TextVector(cfg)
get_vector = tv.get_vec_batch  # ä¿®æ­£åç§°
get_sim = tv.vector_similarity