def process_and_index_files(file_objs: List, kb_name: str = DEFAULT_KB) -> str:
    print("ï¼ï¼ï¼è¿›å…¥äº†æ–‡ä»¶å¤„ç†å‡½æ•°ï¼ï¼ï¼", flush=True)
    """å¤„ç†å¹¶ç´¢å¼•æ–‡ä»¶åˆ°æŒ‡å®šçš„çŸ¥è¯†åº“"""
    # ç¡®ä¿çŸ¥è¯†åº“ç›®å½•å­˜åœ¨
    kb_dir = os.path.join(KB_BASE_DIR, kb_name)
    os.makedirs(kb_dir, exist_ok=True)

    # è®¾ç½®ä¸´æ—¶å¤„ç†æ–‡ä»¶è·¯å¾„
    semantic_chunk_output = os.path.join(OUTPUT_DIR, "semantic_chunk_output.json")
    semantic_chunk_vector = os.path.join(OUTPUT_DIR, "semantic_chunk_vector.json")

    # è®¾ç½®çŸ¥è¯†åº“ç´¢å¼•æ–‡ä»¶è·¯å¾„
    semantic_chunk_index = os.path.join(kb_dir, "semantic_chunk.index")
    semantic_chunk_metadata = os.path.join(kb_dir, "semantic_chunk_metadata.json")

    new_chunks = []  # æ”¹åï¼šè¿™é‡Œå­˜æ”¾æœ¬æ¬¡æ–°å¤„ç†çš„åˆ†å—
    error_messages = []

    try:
        if not file_objs or len(file_objs) == 0:
            return "é”™è¯¯ï¼šæ²¡æœ‰é€‰æ‹©ä»»ä½•æ–‡ä»¶"

        print(f"å¼€å§‹å¤„ç† {len(file_objs)} ä¸ªæ–‡ä»¶ï¼Œç›®æ ‡çŸ¥è¯†åº“: {kb_name}...")

        # 1. å¤šçº¿ç¨‹å¤„ç†æ–‡ä»¶è¯»å–å’Œåˆæ­¥åˆ‡åˆ†
        with ThreadPoolExecutor(max_workers=1) as executor:
            future_to_file = {executor.submit(process_single_file, file_obj.name): file_obj for file_obj in file_objs}
            for future in as_completed(future_to_file):
                result = future.result()
                file_obj = future_to_file[future]
                file_name = file_obj.name

                if isinstance(result, str) and result.startswith("å¤„ç†æ–‡ä»¶"):
                    error_messages.append(result)
                    print(result)
                    continue

                # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºæœ‰æ•ˆæ–‡æœ¬
                if not result or not isinstance(result, str) or len(result.strip()) == 0:
                    error_messages.append(f"æ–‡ä»¶ {file_name} å¤„ç†åå†…å®¹ä¸ºç©º")
                    print(f"è­¦å‘Š: æ–‡ä»¶ {file_name} å¤„ç†åå†…å®¹ä¸ºç©º")
                    continue

                print(f"å¯¹æ–‡ä»¶ {file_name} è¿›è¡Œè¯­ä¹‰åˆ†å—...")
                # è°ƒç”¨ä½ çš„ semantic_chunk å‡½æ•°
                chunks = semantic_chunk(result)

                if not chunks or len(chunks) == 0:
                    error_messages.append(f"æ–‡ä»¶ {file_name} æ— æ³•ç”Ÿæˆä»»ä½•åˆ†å—")
                    print(f"è­¦å‘Š: æ–‡ä»¶ {file_name} æ— æ³•ç”Ÿæˆä»»ä½•åˆ†å—")
                    continue

                # å°†å¤„ç†åçš„æºæ–‡ä»¶å¤åˆ¶ä¿å­˜åˆ°çŸ¥è¯†åº“ç›®å½•
                file_basename = os.path.basename(file_name)
                dest_file_path = os.path.join(kb_dir, file_basename)
                try:
                    shutil.copy2(file_name, dest_file_path)
                    print(f"å·²å°†æ–‡ä»¶ {file_basename} å¤åˆ¶åˆ°çŸ¥è¯†åº“ {kb_name}")
                except Exception as e:
                    print(f"å¤åˆ¶æ–‡ä»¶åˆ°çŸ¥è¯†åº“å¤±è´¥: {str(e)}")

                # ä¸ºè¿™ä¸€æ‰¹ chunks ä¸´æ—¶æ‰“ä¸Šæ–‡ä»¶åæ ‡ç­¾ï¼Œæ–¹ä¾¿åç»­ç”Ÿæˆ ID
                for c in chunks:
                    c["metadata"] = {"source": file_basename}

                new_chunks.extend(chunks)
                print(f"æ–‡ä»¶ {file_name} å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")

        if not new_chunks:
            return "æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥æˆ–å†…å®¹ä¸ºç©º\n" + "\n".join(error_messages)

        # 2. æ¸…æ´—æ–‡æœ¬å¹¶ç”Ÿæˆå”¯ä¸€ ID (æ ¸å¿ƒä¿®æ”¹)
        valid_chunks = []
        import hashlib  # å¼•å…¥ hashlib

        for chunk in new_chunks:
            # æ·±åº¦æ¸…ç†æ–‡æœ¬
            clean_chunk_text = clean_text(chunk["chunk"])
            source_file = chunk.get("metadata", {}).get("source", "unknown")

            # æ£€æŸ¥æ¸…ç†åçš„æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆ
            if clean_chunk_text and 1 <= len(clean_chunk_text) <= 8000:
                chunk["chunk"] = clean_chunk_text

                # ğŸŸ¢ ä¿®æ”¹ç‚¹ï¼šç”Ÿæˆå”¯ä¸€ Hash ID (æ–‡ä»¶å+å†…å®¹)
                # è§£å†³äº†åˆ†æ‰¹ä¸Šä¼  ID å†²çªå’Œé‡ç½®çš„é—®é¢˜
                unique_str = f"{source_file}_{clean_chunk_text}"
                chunk["id"] = hashlib.md5(unique_str.encode('utf-8')).hexdigest()

                valid_chunks.append(chunk)

            elif len(clean_chunk_text) > 8000:
                # æˆªæ–­å¤„ç†
                chunk["chunk"] = clean_chunk_text[:8000]

                # æˆªæ–­ååŒæ ·ç”Ÿæˆ ID
                unique_str = f"{source_file}_{chunk['chunk']}"
                chunk["id"] = hashlib.md5(unique_str.encode('utf-8')).hexdigest()

                valid_chunks.append(chunk)
                print(f"è­¦å‘Š: åˆ†å—è¿‡é•¿å·²è¢«æˆªæ–­ï¼Œæºæ–‡ä»¶: {source_file}")
            else:
                print(f"è­¦å‘Š: è·³è¿‡æ— æ•ˆåˆ†å—")

        if not valid_chunks:
            return "æ‰€æœ‰ç”Ÿæˆçš„åˆ†å—å†…å®¹æ— æ•ˆæˆ–ä¸ºç©º\n" + "\n".join(error_messages)

        print(f"æœ¬æ¬¡æ–°å¢ {len(valid_chunks)} ä¸ªæœ‰æ•ˆåˆ†å—")

        # 3. å¢é‡åˆå¹¶ä¿å­˜ (æ ¸å¿ƒä¿®æ”¹)
        # è¯»å–å·²æœ‰çš„ JSON æ•°æ®ï¼Œé˜²æ­¢è¦†ç›–æ—§æ•°æ®
        final_all_chunks = []
        if os.path.exists(semantic_chunk_output):
            try:
                with open(semantic_chunk_output, 'r', encoding='utf-8') as f:
                    old_data = json.load(f)
                    if isinstance(old_data, list):
                        final_all_chunks = old_data
            except Exception as e:
                print(f"è¯»å–æ—§æ•°æ®å¤±è´¥ï¼Œå°†é‡æ–°åˆ›å»º: {e}")

        # ä½¿ç”¨å­—å…¸å»é‡åˆå¹¶ï¼š{id: chunk_data}
        # å¦‚æœ ID ç›¸åŒï¼ˆå†…å®¹+æ–‡ä»¶åç›¸åŒï¼‰ï¼Œæ–°æ•°æ®ä¼šè¦†ç›–æ—§æ•°æ®
        chunk_map = {item["id"]: item for item in final_all_chunks}

        for item in valid_chunks:
            chunk_map[item["id"]] = item

        # è½¬å›åˆ—è¡¨
        final_all_chunks = list(chunk_map.values())

        # ä¿å­˜åˆå¹¶åçš„å®Œæ•´åˆ—è¡¨
        with open(semantic_chunk_output, 'w', encoding='utf-8') as json_file:
            json.dump(final_all_chunks, json_file, ensure_ascii=False, indent=4)
        print(f"è¯­ä¹‰åˆ†å—å®Œæˆï¼Œå½“å‰åº“ä¸­æ€»è®¡ {len(final_all_chunks)} ä¸ªåˆ†å—ã€‚è·¯å¾„: {semantic_chunk_output}")

        # 4. å‘é‡åŒ– (æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¯¹æ•´ä¸ªåº“è¿›è¡Œå‘é‡åŒ–ï¼Œä¿è¯ç´¢å¼•å®Œæ•´)
        # å¦‚æœæ•°æ®é‡å·¨å¤§ï¼Œåç»­å¯ä¼˜åŒ–ä¸ºåªå‘é‡åŒ–æ–°å¢éƒ¨åˆ†ï¼Œä½†ç›®å‰å…¨é‡æœ€ç¨³
        print(f"å¼€å§‹å‘é‡åŒ–æ‰€æœ‰ {len(final_all_chunks)} ä¸ªåˆ†å—...")
        vectorize_file(final_all_chunks, semantic_chunk_vector)
        print(f"è¯­ä¹‰åˆ†å—å‘é‡åŒ–å®Œæˆ: {semantic_chunk_vector}")

        # éªŒè¯å‘é‡æ–‡ä»¶
        try:
            with open(semantic_chunk_vector, 'r', encoding='utf-8') as f:
                vector_data = json.load(f)

            if not vector_data or len(vector_data) == 0:
                return f"å‘é‡åŒ–å¤±è´¥: ç”Ÿæˆçš„å‘é‡æ–‡ä»¶ä¸ºç©º\n" + "\n".join(error_messages)

            if 'vector' not in vector_data[0]:
                return f"å‘é‡åŒ–å¤±è´¥: æ•°æ®ä¸­ç¼ºå°‘å‘é‡å­—æ®µ\n" + "\n".join(error_messages)

            print(f"æˆåŠŸç”Ÿæˆ {len(vector_data)} ä¸ªå‘é‡")
        except Exception as e:
            return f"è¯»å–å‘é‡æ–‡ä»¶å¤±è´¥: {str(e)}\n" + "\n".join(error_messages)

        # 5. æ„å»ºç´¢å¼•
        print(f"å¼€å§‹ä¸ºçŸ¥è¯†åº“ {kb_name} æ„å»ºç´¢å¼•...")
        build_faiss_index(semantic_chunk_vector, semantic_chunk_index, semantic_chunk_metadata)
        print(f"çŸ¥è¯†åº“ {kb_name} ç´¢å¼•æ„å»ºå®Œæˆ: {semantic_chunk_index}")

        status = f"çŸ¥è¯†åº“ {kb_name} æ›´æ–°æˆåŠŸï¼æœ¬æ¬¡æ–°å¢ {len(valid_chunks)} ä¸ªåˆ†å—ï¼Œåº“ä¸­æ€»è®¡ {len(final_all_chunks)} ä¸ªã€‚\n"
        if error_messages:
            status += "ä»¥ä¸‹æ–‡ä»¶å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼š\n" + "\n".join(error_messages)
        return status

    except Exception as e:
        error = f"çŸ¥è¯†åº“ {kb_name} ç´¢å¼•æ„å»ºè¿‡ç¨‹ä¸­å‡ºé”™ï¼š{str(e)}"
        print(error)
        traceback.print_exc()
        return error + "\n" + "\n".join(error_messages)